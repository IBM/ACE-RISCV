diff --git a/arch/riscv/include/asm/kvm_host.h b/arch/riscv/include/asm/kvm_host.h
index cc7da66ee0c0..d23aa2eb905d 100644
--- a/arch/riscv/include/asm/kvm_host.h
+++ b/arch/riscv/include/asm/kvm_host.h
@@ -232,6 +232,12 @@ struct kvm_vcpu_arch {
 
 	/* Performance monitoring context */
 	struct kvm_pmu pmu_context;
+
+	// ACE START
+	bool is_confidential_vm;
+	unsigned long confidential_vm_id;
+	unsigned long vcpu_id;
+	// ACE END
 };
 
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
@@ -322,6 +328,7 @@ int kvm_riscv_vcpu_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 			struct kvm_cpu_trap *trap);
 
 void __kvm_riscv_switch_to(struct kvm_vcpu_arch *vcpu_arch);
+void __kvm_riscv_ace_switch_to(struct kvm_vcpu_arch *vcpu_arch, long fid, long arg0, long arg1);
 
 int kvm_riscv_vcpu_set_interrupt(struct kvm_vcpu *vcpu, unsigned int irq);
 int kvm_riscv_vcpu_unset_interrupt(struct kvm_vcpu *vcpu, unsigned int irq);
diff --git a/arch/riscv/include/asm/kvm_vcpu_sbi.h b/arch/riscv/include/asm/kvm_vcpu_sbi.h
index 8425556af7d1..e7fd6c4d0182 100644
--- a/arch/riscv/include/asm/kvm_vcpu_sbi.h
+++ b/arch/riscv/include/asm/kvm_vcpu_sbi.h
@@ -59,5 +59,6 @@ extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_srst;
 extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_hsm;
 extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_experimental;
 extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_vendor;
+extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_ace;
 
 #endif /* __RISCV_KVM_VCPU_SBI_H__ */
diff --git a/arch/riscv/include/asm/sbi.h b/arch/riscv/include/asm/sbi.h
index 945b7be249c1..87871324865b 100644
--- a/arch/riscv/include/asm/sbi.h
+++ b/arch/riscv/include/asm/sbi.h
@@ -30,6 +30,7 @@ enum sbi_ext_id {
 	SBI_EXT_HSM = 0x48534D,
 	SBI_EXT_SRST = 0x53525354,
 	SBI_EXT_PMU = 0x504D55,
+	SBI_EXT_ACE = 0x509999,
 
 	/* Experimentals extensions must lie within this range */
 	SBI_EXT_EXPERIMENTAL_START = 0x08000000,
diff --git a/arch/riscv/kvm/Makefile b/arch/riscv/kvm/Makefile
index 278e97c06e0a..56bfdde53fa4 100644
--- a/arch/riscv/kvm/Makefile
+++ b/arch/riscv/kvm/Makefile
@@ -25,4 +25,5 @@ kvm-y += vcpu_sbi_base.o
 kvm-y += vcpu_sbi_replace.o
 kvm-y += vcpu_sbi_hsm.o
 kvm-y += vcpu_timer.o
+kvm-y += vcpu_sbi_ace.o
 kvm-$(CONFIG_RISCV_PMU_SBI) += vcpu_pmu.o vcpu_sbi_pmu.o
diff --git a/arch/riscv/kvm/vcpu.c b/arch/riscv/kvm/vcpu.c
index 7d010b0be54e..f0c12fe4c3ff 100644
--- a/arch/riscv/kvm/vcpu.c
+++ b/arch/riscv/kvm/vcpu.c
@@ -982,10 +982,17 @@ static void kvm_riscv_update_hvip(struct kvm_vcpu *vcpu)
  */
 static void noinstr kvm_riscv_vcpu_enter_exit(struct kvm_vcpu *vcpu)
 {
-	guest_state_enter_irqoff();
-	__kvm_riscv_switch_to(&vcpu->arch);
-	vcpu->arch.last_exit_cpu = vcpu->cpu;
-	guest_state_exit_irqoff();
+	if (vcpu->arch.is_confidential_vm) {
+		guest_state_enter_irqoff();
+		__kvm_riscv_ace_switch_to(&vcpu->arch, 1010, vcpu->arch.confidential_vm_id, vcpu->arch.vcpu_id);
+		vcpu->arch.last_exit_cpu = vcpu->cpu;
+		guest_state_exit_irqoff();
+	} else {
+		guest_state_enter_irqoff();
+		__kvm_riscv_switch_to(&vcpu->arch);
+		vcpu->arch.last_exit_cpu = vcpu->cpu;
+		guest_state_exit_irqoff();
+	}
 }
 
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
@@ -1100,6 +1107,11 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		trap.stval = csr_read(CSR_STVAL);
 		trap.htval = csr_read(CSR_HTVAL);
 		trap.htinst = csr_read(CSR_HTINST);
+		if (vcpu->arch.is_confidential_vm) {
+			// this is a hack in which we store the instruction in the vsscratch
+			// His will change once we move to CoVE approach of using the NACL extension
+			trap.htinst = csr_read(CSR_VSSCRATCH);
+		} 
 
 		/* Syncup interrupts state with HW */
 		kvm_riscv_vcpu_sync_interrupts(vcpu);
diff --git a/arch/riscv/kvm/vcpu_exit.c b/arch/riscv/kvm/vcpu_exit.c
index 4ea101a73d8b..21c9e8d9e6cc 100644
--- a/arch/riscv/kvm/vcpu_exit.c
+++ b/arch/riscv/kvm/vcpu_exit.c
@@ -181,6 +181,7 @@ int kvm_riscv_vcpu_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	/* Handle guest traps */
 	ret = -EFAULT;
 	run->exit_reason = KVM_EXIT_UNKNOWN;
+
 	switch (trap->scause) {
 	case EXC_INST_ILLEGAL:
 		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV) {
diff --git a/arch/riscv/kvm/vcpu_insn.c b/arch/riscv/kvm/vcpu_insn.c
index f689337b78ff..fa25d3f11e0e 100644
--- a/arch/riscv/kvm/vcpu_insn.c
+++ b/arch/riscv/kvm/vcpu_insn.c
@@ -417,8 +417,9 @@ int kvm_riscv_vcpu_virtual_insn(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		if (insn == 0) {
 			ct = &vcpu->arch.guest_context;
 			insn = kvm_riscv_vcpu_unpriv_read(vcpu, true,
-							  ct->sepc,
-							  &utrap);
+							ct->sepc,
+							&utrap);
+
 			if (utrap.scause) {
 				utrap.sepc = ct->sepc;
 				kvm_riscv_vcpu_trap_redirect(vcpu, &utrap);
@@ -472,14 +473,13 @@ int kvm_riscv_vcpu_mmio_load(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		 * Bit[0] == 0 implies trapped instruction value is
 		 * zero or special value.
 		 */
-		insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc,
-						  &utrap);
+		insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc, &utrap);
 		if (utrap.scause) {
 			/* Redirect trap if we failed to read instruction */
 			utrap.sepc = ct->sepc;
 			kvm_riscv_vcpu_trap_redirect(vcpu, &utrap);
 			return 1;
-		}
+		}			
 		insn_len = INSN_LEN(insn);
 	}
 
@@ -506,20 +506,20 @@ int kvm_riscv_vcpu_mmio_load(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	} else if ((insn & INSN_MASK_LHU) == INSN_MATCH_LHU) {
 		len = 2;
 #ifdef CONFIG_64BIT
-	} else if ((insn & INSN_MASK_C_LD) == INSN_MATCH_C_LD) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_LD) == INSN_MATCH_C_LD) {
 		len = 8;
 		shift = 8 * (sizeof(ulong) - len);
 		insn = RVC_RS2S(insn) << SH_RD;
-	} else if ((insn & INSN_MASK_C_LDSP) == INSN_MATCH_C_LDSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_LDSP) == INSN_MATCH_C_LDSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 8;
 		shift = 8 * (sizeof(ulong) - len);
 #endif
-	} else if ((insn & INSN_MASK_C_LW) == INSN_MATCH_C_LW) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_LW) == INSN_MATCH_C_LW) {
 		len = 4;
 		shift = 8 * (sizeof(ulong) - len);
 		insn = RVC_RS2S(insn) << SH_RD;
-	} else if ((insn & INSN_MASK_C_LWSP) == INSN_MATCH_C_LWSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_LWSP) == INSN_MATCH_C_LWSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 4;
 		shift = 8 * (sizeof(ulong) - len);
@@ -575,7 +575,7 @@ int kvm_riscv_vcpu_mmio_store(struct kvm_vcpu *vcpu, struct kvm_run *run,
 			      unsigned long fault_addr,
 			      unsigned long htinst)
 {
-	u8 data8;
+	u8 data8; 
 	u16 data16;
 	u32 data32;
 	u64 data64;
@@ -598,8 +598,7 @@ int kvm_riscv_vcpu_mmio_store(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		 * Bit[0] == 0 implies trapped instruction value is
 		 * zero or special value.
 		 */
-		insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc,
-						  &utrap);
+		insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc, &utrap);
 		if (utrap.scause) {
 			/* Redirect trap if we failed to read instruction */
 			utrap.sepc = ct->sepc;
@@ -623,18 +622,18 @@ int kvm_riscv_vcpu_mmio_store(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	} else if ((insn & INSN_MASK_SH) == INSN_MATCH_SH) {
 		len = 2;
 #ifdef CONFIG_64BIT
-	} else if ((insn & INSN_MASK_C_SD) == INSN_MATCH_C_SD) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_SD) == INSN_MATCH_C_SD) {
 		len = 8;
 		data64 = GET_RS2S(insn, &vcpu->arch.guest_context);
-	} else if ((insn & INSN_MASK_C_SDSP) == INSN_MATCH_C_SDSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_SDSP) == INSN_MATCH_C_SDSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 8;
 		data64 = GET_RS2C(insn, &vcpu->arch.guest_context);
 #endif
-	} else if ((insn & INSN_MASK_C_SW) == INSN_MATCH_C_SW) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_SW) == INSN_MATCH_C_SW) {
 		len = 4;
 		data32 = GET_RS2S(insn, &vcpu->arch.guest_context);
-	} else if ((insn & INSN_MASK_C_SWSP) == INSN_MATCH_C_SWSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_SWSP) == INSN_MATCH_C_SWSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 4;
 		data32 = GET_RS2C(insn, &vcpu->arch.guest_context);
@@ -750,4 +749,4 @@ int kvm_riscv_vcpu_mmio_return(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	vcpu->arch.guest_context.sepc += vcpu->arch.mmio_decode.insn_len;
 
 	return 0;
-}
+}
\ No newline at end of file
diff --git a/arch/riscv/kvm/vcpu_sbi.c b/arch/riscv/kvm/vcpu_sbi.c
index 15fde15f9fb8..97df2351feb2 100644
--- a/arch/riscv/kvm/vcpu_sbi.c
+++ b/arch/riscv/kvm/vcpu_sbi.c
@@ -41,6 +41,7 @@ static const struct kvm_vcpu_sbi_extension *sbi_ext[] = {
 	&vcpu_sbi_ext_pmu,
 	&vcpu_sbi_ext_experimental,
 	&vcpu_sbi_ext_vendor,
+	&vcpu_sbi_ext_ace,
 };
 
 void kvm_riscv_vcpu_sbi_forward(struct kvm_vcpu *vcpu, struct kvm_run *run)
@@ -69,6 +70,10 @@ void kvm_riscv_vcpu_sbi_system_reset(struct kvm_vcpu *vcpu,
 	unsigned long i;
 	struct kvm_vcpu *tmp;
 
+	if (vcpu->arch.is_confidential_vm) {
+		vcpu->arch.is_confidential_vm = 0;
+	}
+
 	kvm_for_each_vcpu(i, tmp, vcpu->kvm)
 		tmp->arch.power_off = true;
 	kvm_make_all_cpus_request(vcpu->kvm, KVM_REQ_SLEEP);
@@ -127,6 +132,7 @@ int kvm_riscv_vcpu_sbi_ecall(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	bool ext_is_v01 = false;
 
 	sbi_ext = kvm_vcpu_sbi_find_ext(cp->a7);
+
 	if (sbi_ext && sbi_ext->handler) {
 #ifdef CONFIG_RISCV_SBI_V01
 		if (cp->a7 >= SBI_EXT_0_1_SET_TIMER &&
diff --git a/arch/riscv/kvm/vcpu_sbi_ace.c b/arch/riscv/kvm/vcpu_sbi_ace.c
new file mode 100644
index 000000000000..6e52e563bce0
--- /dev/null
+++ b/arch/riscv/kvm/vcpu_sbi_ace.c
@@ -0,0 +1,134 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2021 IBM.
+ *
+ * Authors:
+ *     Wojciech Ozga <woz@zurich.ibm.com>
+ */
+
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <linux/kvm_host.h>
+#include <asm/sbi.h>
+#include <asm/kvm_vcpu_sbi.h>
+#include <asm/kvm_host.h>
+#include <asm/io.h>
+
+const int SECURITY_MONITOR_EXTID = 0x510000;
+const int SECURITY_MONITOR_PAGE_IN_FID = 2003;
+
+const int SBI_EXT_ACE_LOAD_ALL_PAGES = 0;
+const int SBI_EXT_ACE_REGISTER_SVM = 1;
+const int SBI_EXT_ACE_PAGE_IN = 2;
+
+phys_addr_t test_phys_addr = 0;
+
+static int kvm_sbi_ace_load_all_pages(struct kvm_vcpu *vcpu)
+{
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	struct kvm_memory_slot *memslot;
+	unsigned long hva, fault_addr;
+	bool writable;
+	gfn_t gfn;
+	int ret;
+	long page;
+	unsigned long memory_size = cp->a0;
+
+	printk(KERN_INFO "ACE KVM: Loading all pages\n");
+							     //80f75428
+	unsigned long memory_start = 0x80000000;
+	// unsigned long number_of_pages = memory_size / 4096; 
+	unsigned long number_of_pages = 1024*1024; // ~4GiB because a page is 4KiB;
+	for (page=0; page<number_of_pages; page++) {
+		fault_addr = memory_start + page * 4096;
+		gfn = fault_addr >> PAGE_SHIFT;
+		memslot = gfn_to_memslot(vcpu->kvm, gfn);
+		hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+		if (memslot != NULL && !kvm_is_error_hva(hva)) {
+			kvm_riscv_gstage_map(vcpu, memslot, fault_addr, hva, true);
+		}
+	}
+
+	return 0;
+}
+
+static int kvm_sbi_ace_register_svm(struct kvm_vcpu *vcpu)
+{
+	struct kvm *kvm = vcpu->kvm;
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	unsigned long confidential_vm_id = cp->a0;
+	unsigned long vcpu_id = cp->a1;
+
+	vcpu->arch.is_confidential_vm = 1;
+	vcpu->arch.confidential_vm_id = confidential_vm_id;
+	vcpu->arch.vcpu_id = vcpu_id;
+
+	printk(KERN_INFO "ACE KVM: registered new confidential VM id=%ld\n", confidential_vm_id);
+
+	return 0;
+}
+
+static int kvm_sbi_ace_page_in(struct kvm_vcpu *vcpu, struct kvm_vcpu_sbi_return *retdata)
+{
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	unsigned long confidential_vm_id = vcpu->arch.confidential_vm_id;
+	unsigned long hart_id = vcpu->arch.vcpu_id;
+	unsigned long virt_addr = cp->a0;
+	unsigned long is_error = 0;
+	int page_size = 4096;
+	struct sbiret result;
+	bool writable;
+
+	gpa_t gpa = virt_addr;
+	gfn_t gfn = gpa >> PAGE_SHIFT;
+
+	struct kvm_memory_slot *memslot = gfn_to_memslot(vcpu->kvm, gfn);
+	phys_addr_t hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+	kvm_riscv_gstage_map(vcpu, memslot, gpa, hva, true);
+	unsigned long hfn = gfn_to_pfn_prot(vcpu->kvm, gfn, true, NULL);
+	phys_addr_t hpa = hfn << PAGE_SHIFT;
+
+	struct kvm_cpu_context *reset_cntx = &vcpu->arch.guest_reset_context;
+	retdata->out_val = hpa;
+
+	return 0;
+}
+
+static int kvm_sbi_ext_ace_handler(struct kvm_vcpu *vcpu, struct kvm_run *run,
+				   struct kvm_vcpu_sbi_return *retdata)
+{
+	int ret = 0;
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	struct kvm *kvm = vcpu->kvm;
+	unsigned long funcid = cp->a6;
+
+	switch (funcid) {
+	case SBI_EXT_ACE_LOAD_ALL_PAGES:
+		mutex_lock(&kvm->lock);
+		ret = kvm_sbi_ace_load_all_pages(vcpu);
+		mutex_unlock(&kvm->lock);
+		break;
+	case SBI_EXT_ACE_REGISTER_SVM:
+		mutex_lock(&kvm->lock);
+		ret = kvm_sbi_ace_register_svm(vcpu);
+		mutex_unlock(&kvm->lock);
+		break;
+	case SBI_EXT_ACE_PAGE_IN:
+		mutex_lock(&kvm->lock);
+		ret = kvm_sbi_ace_page_in(vcpu, retdata);
+		mutex_unlock(&kvm->lock);
+		break;
+	default:
+		ret = SBI_ERR_NOT_SUPPORTED;
+	}
+
+	retdata->err_val = ret;
+
+	return 0;
+}
+
+const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_ace = {
+	.extid_start = SBI_EXT_ACE,
+	.extid_end = SBI_EXT_ACE,
+	.handler = kvm_sbi_ext_ace_handler,
+};
diff --git a/arch/riscv/kvm/vcpu_sbi_hsm.c b/arch/riscv/kvm/vcpu_sbi_hsm.c
index 7dca0e9381d9..46f33f29d411 100644
--- a/arch/riscv/kvm/vcpu_sbi_hsm.c
+++ b/arch/riscv/kvm/vcpu_sbi_hsm.c
@@ -32,8 +32,15 @@ static int kvm_sbi_hsm_vcpu_start(struct kvm_vcpu *vcpu)
 	reset_cntx->a0 = target_vcpuid;
 	/* private data passed from kernel */
 	reset_cntx->a1 = cp->a2;
-	kvm_make_request(KVM_REQ_VCPU_RESET, target_vcpu);
 
+	if (vcpu->arch.is_confidential_vm) {
+		printk(KERN_INFO "ACE KVM: starting new vcpu\n");
+		target_vcpu->arch.is_confidential_vm = 1;
+		target_vcpu->arch.confidential_vm_id = vcpu->arch.confidential_vm_id;
+		target_vcpu->arch.vcpu_id = target_vcpuid;
+	}
+
+	kvm_make_request(KVM_REQ_VCPU_RESET, target_vcpu);
 	kvm_riscv_vcpu_power_on(target_vcpu);
 
 	return 0;
diff --git a/arch/riscv/kvm/vcpu_switch.S b/arch/riscv/kvm/vcpu_switch.S
index d74df8eb4d71..3ff579a9a882 100644
--- a/arch/riscv/kvm/vcpu_switch.S
+++ b/arch/riscv/kvm/vcpu_switch.S
@@ -210,6 +210,124 @@ __kvm_switch_return:
 	ret
 ENDPROC(__kvm_riscv_switch_to)
 
+# ACE START,
+#	a0 - address of the vcpu->arch
+#	a1 - SM-call function ID
+#   a2 - first argument of the SM-call
+.align 4
+ENTRY(__kvm_riscv_ace_switch_to)
+	/* Save Host GPRs (except A0 and T0-T6) */
+	REG_S	ra, (KVM_ARCH_HOST_RA)(a0)
+	REG_S	sp, (KVM_ARCH_HOST_SP)(a0)
+	REG_S	gp, (KVM_ARCH_HOST_GP)(a0)
+	REG_S	tp, (KVM_ARCH_HOST_TP)(a0)
+	REG_S	s0, (KVM_ARCH_HOST_S0)(a0)
+	REG_S	s1, (KVM_ARCH_HOST_S1)(a0)
+	REG_S	a1, (KVM_ARCH_HOST_A1)(a0)
+	REG_S	a2, (KVM_ARCH_HOST_A2)(a0)
+	REG_S	a3, (KVM_ARCH_HOST_A3)(a0)
+	REG_S	a4, (KVM_ARCH_HOST_A4)(a0)
+	REG_S	a5, (KVM_ARCH_HOST_A5)(a0)
+	REG_S	a6, (KVM_ARCH_HOST_A6)(a0)
+	REG_S	a7, (KVM_ARCH_HOST_A7)(a0)
+	REG_S	s2, (KVM_ARCH_HOST_S2)(a0)
+	REG_S	s3, (KVM_ARCH_HOST_S3)(a0)
+	REG_S	s4, (KVM_ARCH_HOST_S4)(a0)
+	REG_S	s5, (KVM_ARCH_HOST_S5)(a0)
+	REG_S	s6, (KVM_ARCH_HOST_S6)(a0)
+	REG_S	s7, (KVM_ARCH_HOST_S7)(a0)
+	REG_S	s8, (KVM_ARCH_HOST_S8)(a0)
+	REG_S	s9, (KVM_ARCH_HOST_S9)(a0)
+	REG_S	s10, (KVM_ARCH_HOST_S10)(a0)
+	REG_S	s11, (KVM_ARCH_HOST_S11)(a0)
+
+	/* Load Guest CSR values */
+	REG_L	t0, (KVM_ARCH_GUEST_SSTATUS)(a0)
+	REG_L	t1, (KVM_ARCH_GUEST_HSTATUS)(a0)
+	REG_L	t2, (KVM_ARCH_GUEST_SCOUNTEREN)(a0)
+	la	t4, __kvm_switch_return
+	REG_L	t5, (KVM_ARCH_GUEST_SEPC)(a0)
+
+	/* Save Host and Restore Guest SSTATUS */
+	csrrw	t0, CSR_SSTATUS, t0
+
+	/* Save Host and Restore Guest HSTATUS */
+	csrrw	t1, CSR_HSTATUS, t1
+
+	/* Save Host and Restore Guest SCOUNTEREN */
+	csrrw	t2, CSR_SCOUNTEREN, t2
+
+	/* Save Host STVEC and change it to return path */
+	csrrw	t4, CSR_STVEC, t4
+
+	/* Save Host SSCRATCH and change it to struct kvm_vcpu_arch pointer */
+	csrrw	t3, CSR_SSCRATCH, a0
+
+	/* Restore Guest SEPC */
+	csrw	CSR_SEPC, t5
+
+	/* Store Host CSR values */
+	REG_S	t0, (KVM_ARCH_HOST_SSTATUS)(a0)
+	REG_S	t1, (KVM_ARCH_HOST_HSTATUS)(a0)
+	REG_S	t2, (KVM_ARCH_HOST_SCOUNTEREN)(a0)
+	REG_S	t3, (KVM_ARCH_HOST_SSCRATCH)(a0)
+	REG_S	t4, (KVM_ARCH_HOST_STVEC)(a0)
+
+	# Invoke security monitor call. We cannot use
+	# GPRs because they might carry SBI- or MMIO-related 
+	# responses. Our hackish (temporal) solution is to
+	# use vs* CSRs to store the original GPRs values
+	# and restore them in the security monitor.
+	REG_L	t0, (KVM_ARCH_GUEST_A7)(a0)
+	csrw	CSR_VSTVAL, t0
+	REG_L	t0, (KVM_ARCH_GUEST_A6)(a0)
+	csrw	CSR_VSEPC, t0
+	csrw	CSR_VSTVEC, a2
+	csrw	CSR_VSSCRATCH, a3
+
+	li		a7, 0x510000 # ACE_EXT_ID that identifies SM-call
+	add		a6, a1, 0	 # function ID
+
+	/* Restore Guest GPRs (except A0) */
+	REG_L	ra, (KVM_ARCH_GUEST_RA)(a0)
+	REG_L	sp, (KVM_ARCH_GUEST_SP)(a0)
+	REG_L	gp, (KVM_ARCH_GUEST_GP)(a0)
+	REG_L	tp, (KVM_ARCH_GUEST_TP)(a0)
+	REG_L	t0, (KVM_ARCH_GUEST_T0)(a0)
+	REG_L	t1, (KVM_ARCH_GUEST_T1)(a0)
+	REG_L	t2, (KVM_ARCH_GUEST_T2)(a0)
+	REG_L	s0, (KVM_ARCH_GUEST_S0)(a0)
+	REG_L	s1, (KVM_ARCH_GUEST_S1)(a0)
+	REG_L	a1, (KVM_ARCH_GUEST_A1)(a0)
+	REG_L	a2, (KVM_ARCH_GUEST_A2)(a0)
+	REG_L	a3, (KVM_ARCH_GUEST_A3)(a0)
+	REG_L	a4, (KVM_ARCH_GUEST_A4)(a0)
+	REG_L	a5, (KVM_ARCH_GUEST_A5)(a0)
+	# REG_L	a6, (KVM_ARCH_GUEST_A6)(a0)
+	# REG_L	a7, (KVM_ARCH_GUEST_A7)(a0)
+	REG_L	s2, (KVM_ARCH_GUEST_S2)(a0)
+	REG_L	s3, (KVM_ARCH_GUEST_S3)(a0)
+	REG_L	s4, (KVM_ARCH_GUEST_S4)(a0)
+	REG_L	s5, (KVM_ARCH_GUEST_S5)(a0)
+	REG_L	s6, (KVM_ARCH_GUEST_S6)(a0)
+	REG_L	s7, (KVM_ARCH_GUEST_S7)(a0)
+	REG_L	s8, (KVM_ARCH_GUEST_S8)(a0)
+	REG_L	s9, (KVM_ARCH_GUEST_S9)(a0)
+	REG_L	s10, (KVM_ARCH_GUEST_S10)(a0)
+	REG_L	s11, (KVM_ARCH_GUEST_S11)(a0)
+	REG_L	t3, (KVM_ARCH_GUEST_T3)(a0)
+	REG_L	t4, (KVM_ARCH_GUEST_T4)(a0)	
+	REG_L	t5, (KVM_ARCH_GUEST_T5)(a0)
+	REG_L	t6, (KVM_ARCH_GUEST_T6)(a0)	
+
+	REG_L	a0, (KVM_ARCH_GUEST_A0)(a0)
+
+	/* Resume Guest */
+	ecall
+ENDPROC(__kvm_riscv_ace_switch_to)
+
+# ACE END
+
 ENTRY(__kvm_riscv_unpriv_trap)
 	/*
 	 * We assume that faulting unpriv load/store instruction is
diff --git a/rust/alloc/alloc.rs b/rust/alloc/alloc.rs
index ca224a541770..65ce3fea232a 100644
--- a/rust/alloc/alloc.rs
+++ b/rust/alloc/alloc.rs
@@ -31,13 +31,13 @@ extern "Rust" {
     // like `malloc`, `realloc`, and `free`, respectively.
     #[rustc_allocator]
     #[rustc_allocator_nounwind]
-    fn __rust_alloc(size: usize, align: usize) -> *mut u8;
+    fn __rust_alloc(size: usize, align: usize) -> *mut usize;
     #[rustc_allocator_nounwind]
-    fn __rust_dealloc(ptr: *mut u8, size: usize, align: usize);
+    fn __rust_dealloc(ptr: *mut usize, size: usize, align: usize);
     #[rustc_allocator_nounwind]
-    fn __rust_realloc(ptr: *mut u8, old_size: usize, align: usize, new_size: usize) -> *mut u8;
+    fn __rust_realloc(ptr: *mut usize, old_size: usize, align: usize, new_size: usize) -> *mut usize;
     #[rustc_allocator_nounwind]
-    fn __rust_alloc_zeroed(size: usize, align: usize) -> *mut u8;
+    fn __rust_alloc_zeroed(size: usize, align: usize) -> *mut usize;
 }
 
 /// The global memory allocator.
@@ -87,7 +87,7 @@ pub use std::alloc::Global;
 #[stable(feature = "global_alloc", since = "1.28.0")]
 #[must_use = "losing the pointer will leak memory"]
 #[inline]
-pub unsafe fn alloc(layout: Layout) -> *mut u8 {
+pub unsafe fn alloc(layout: Layout) -> *mut usize {
     unsafe { __rust_alloc(layout.size(), layout.align()) }
 }
 
@@ -105,7 +105,7 @@ pub unsafe fn alloc(layout: Layout) -> *mut u8 {
 /// See [`GlobalAlloc::dealloc`].
 #[stable(feature = "global_alloc", since = "1.28.0")]
 #[inline]
-pub unsafe fn dealloc(ptr: *mut u8, layout: Layout) {
+pub unsafe fn dealloc(ptr: *mut usize, layout: Layout) {
     unsafe { __rust_dealloc(ptr, layout.size(), layout.align()) }
 }
 
@@ -124,7 +124,7 @@ pub unsafe fn dealloc(ptr: *mut u8, layout: Layout) {
 #[stable(feature = "global_alloc", since = "1.28.0")]
 #[must_use = "losing the pointer will leak memory"]
 #[inline]
-pub unsafe fn realloc(ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {
+pub unsafe fn realloc(ptr: *mut usize, layout: Layout, new_size: usize) -> *mut usize {
     unsafe { __rust_realloc(ptr, layout.size(), layout.align(), new_size) }
 }
 
@@ -158,7 +158,7 @@ pub unsafe fn realloc(ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8
 #[stable(feature = "global_alloc", since = "1.28.0")]
 #[must_use = "losing the pointer will leak memory"]
 #[inline]
-pub unsafe fn alloc_zeroed(layout: Layout) -> *mut u8 {
+pub unsafe fn alloc_zeroed(layout: Layout) -> *mut usize {
     unsafe { __rust_alloc_zeroed(layout.size(), layout.align()) }
 }
 
@@ -317,7 +317,7 @@ unsafe impl Allocator for Global {
 #[cfg(all(not(no_global_oom_handling), not(test)))]
 #[lang = "exchange_malloc"]
 #[inline]
-unsafe fn exchange_malloc(size: usize, align: usize) -> *mut u8 {
+unsafe fn exchange_malloc(size: usize, align: usize) -> *mut usize {
     let layout = unsafe { Layout::from_size_align_unchecked(size, align) };
     match Global.allocate(layout) {
         Ok(ptr) => ptr.as_mut_ptr(),
diff --git a/rust/alloc/boxed.rs b/rust/alloc/boxed.rs
index dcfe87b14f3a..309506c36080 100644
--- a/rust/alloc/boxed.rs
+++ b/rust/alloc/boxed.rs
@@ -1020,7 +1020,7 @@ impl<T: ?Sized, A: Allocator> Box<T, A> {
     /// let p = Box::into_raw(x);
     /// unsafe {
     ///     ptr::drop_in_place(p);
-    ///     dealloc(p as *mut u8, Layout::new::<String>());
+    ///     dealloc(p as *mut usize, Layout::new::<String>());
     /// }
     /// ```
     ///
diff --git a/rust/alloc/vec/mod.rs b/rust/alloc/vec/mod.rs
index f77c7368d534..c7d756a82276 100644
--- a/rust/alloc/vec/mod.rs
+++ b/rust/alloc/vec/mod.rs
@@ -1313,7 +1313,7 @@ impl<T, A: Allocator> Vec<T, A> {
     /// # extern "C" {
     /// #     fn deflateGetDictionary(
     /// #         strm: *mut std::ffi::c_void,
-    /// #         dictionary: *mut u8,
+    /// #         dictionary: *mut usize,
     /// #         dictLength: *mut usize,
     /// #     ) -> i32;
     /// # }
diff --git a/rust/kernel/allocator.rs b/rust/kernel/allocator.rs
index 397a3dd57a9b..f3cea1551117 100644
--- a/rust/kernel/allocator.rs
+++ b/rust/kernel/allocator.rs
@@ -10,13 +10,13 @@ use crate::bindings;
 struct KernelAllocator;
 
 unsafe impl GlobalAlloc for KernelAllocator {
-    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
+    unsafe fn alloc(&self, layout: Layout) -> *mut usize {
         // `krealloc()` is used instead of `kmalloc()` because the latter is
         // an inline function and cannot be bound to as a result.
-        unsafe { bindings::krealloc(ptr::null(), layout.size(), bindings::GFP_KERNEL) as *mut u8 }
+        unsafe { bindings::krealloc(ptr::null(), layout.size(), bindings::GFP_KERNEL) as *mut usize }
     }
 
-    unsafe fn dealloc(&self, ptr: *mut u8, _layout: Layout) {
+    unsafe fn dealloc(&self, ptr: *mut usize, _layout: Layout) {
         unsafe {
             bindings::kfree(ptr as *const core::ffi::c_void);
         }
@@ -32,33 +32,33 @@ static ALLOCATOR: KernelAllocator = KernelAllocator;
 //
 // Note that `#[no_mangle]` implies exported too, nowadays.
 #[no_mangle]
-fn __rust_alloc(size: usize, _align: usize) -> *mut u8 {
-    unsafe { bindings::krealloc(core::ptr::null(), size, bindings::GFP_KERNEL) as *mut u8 }
+fn __rust_alloc(size: usize, _align: usize) -> *mut usize {
+    unsafe { bindings::krealloc(core::ptr::null(), size, bindings::GFP_KERNEL) as *mut usize }
 }
 
 #[no_mangle]
-fn __rust_dealloc(ptr: *mut u8, _size: usize, _align: usize) {
+fn __rust_dealloc(ptr: *mut usize, _size: usize, _align: usize) {
     unsafe { bindings::kfree(ptr as *const core::ffi::c_void) };
 }
 
 #[no_mangle]
-fn __rust_realloc(ptr: *mut u8, _old_size: usize, _align: usize, new_size: usize) -> *mut u8 {
+fn __rust_realloc(ptr: *mut usize, _old_size: usize, _align: usize, new_size: usize) -> *mut usize {
     unsafe {
         bindings::krealloc(
             ptr as *const core::ffi::c_void,
             new_size,
             bindings::GFP_KERNEL,
-        ) as *mut u8
+        ) as *mut usize
     }
 }
 
 #[no_mangle]
-fn __rust_alloc_zeroed(size: usize, _align: usize) -> *mut u8 {
+fn __rust_alloc_zeroed(size: usize, _align: usize) -> *mut usize {
     unsafe {
         bindings::krealloc(
             core::ptr::null(),
             size,
             bindings::GFP_KERNEL | bindings::__GFP_ZERO,
-        ) as *mut u8
+        ) as *mut usize
     }
 }
diff --git a/rust/kernel/str.rs b/rust/kernel/str.rs
index b771310fa4a4..f3e0c2035f86 100644
--- a/rust/kernel/str.rs
+++ b/rust/kernel/str.rs
@@ -407,7 +407,7 @@ impl RawFormatter {
     ///
     /// If `pos` is less than `end`, then the region between `pos` (inclusive) and `end`
     /// (exclusive) must be valid for writes for the lifetime of the returned [`RawFormatter`].
-    pub(crate) unsafe fn from_ptrs(pos: *mut u8, end: *mut u8) -> Self {
+    pub(crate) unsafe fn from_ptrs(pos: *mut usize, end: *mut usize) -> Self {
         // INVARIANT: The safety requierments guarantee the type invariants.
         Self {
             beg: pos as _,
@@ -422,7 +422,7 @@ impl RawFormatter {
     ///
     /// The memory region starting at `buf` and extending for `len` bytes must be valid for writes
     /// for the lifetime of the returned [`RawFormatter`].
-    pub(crate) unsafe fn from_buffer(buf: *mut u8, len: usize) -> Self {
+    pub(crate) unsafe fn from_buffer(buf: *mut usize, len: usize) -> Self {
         let pos = buf as usize;
         // INVARIANT: We ensure that `end` is never less then `buf`, and the safety requirements
         // guarantees that the memory region is valid for writes.
@@ -436,7 +436,7 @@ impl RawFormatter {
     /// Returns the current insert position.
     ///
     /// N.B. It may point to invalid memory.
-    pub(crate) fn pos(&self) -> *mut u8 {
+    pub(crate) fn pos(&self) -> *mut usize {
         self.pos as _
     }
 
@@ -461,7 +461,7 @@ impl fmt::Write for RawFormatter {
             unsafe {
                 core::ptr::copy_nonoverlapping(
                     s.as_bytes().as_ptr(),
-                    self.pos as *mut u8,
+                    self.pos as *mut usize,
                     len_to_copy,
                 )
             };
@@ -484,7 +484,7 @@ impl Formatter {
     ///
     /// The memory region starting at `buf` and extending for `len` bytes must be valid for writes
     /// for the lifetime of the returned [`Formatter`].
-    pub(crate) unsafe fn from_buffer(buf: *mut u8, len: usize) -> Self {
+    pub(crate) unsafe fn from_buffer(buf: *mut usize, len: usize) -> Self {
         // SAFETY: The safety requirements of this function satisfy those of the callee.
         Self(unsafe { RawFormatter::from_buffer(buf, len) })
     }
