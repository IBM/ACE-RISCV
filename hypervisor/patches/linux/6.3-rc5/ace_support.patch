diff --git a/arch/riscv/include/asm/kvm_host.h b/arch/riscv/include/asm/kvm_host.h
index cc7da66ee0c0..3e1ab6d5cf55 100644
--- a/arch/riscv/include/asm/kvm_host.h
+++ b/arch/riscv/include/asm/kvm_host.h
@@ -154,6 +154,7 @@ struct kvm_vcpu_csr {
 	unsigned long hvip;
 	unsigned long vsatp;
 	unsigned long scounteren;
+	unsigned long htinst;
 };
 
 struct kvm_vcpu_arch {
@@ -232,6 +233,12 @@ struct kvm_vcpu_arch {
 
 	/* Performance monitoring context */
 	struct kvm_pmu pmu_context;
+
+	// ACE START
+	bool is_confidential_vm;
+	unsigned long confidential_vm_id;
+	unsigned long vcpu_id;
+	// ACE END
 };
 
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
@@ -322,6 +329,7 @@ int kvm_riscv_vcpu_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 			struct kvm_cpu_trap *trap);
 
 void __kvm_riscv_switch_to(struct kvm_vcpu_arch *vcpu_arch);
+void __kvm_riscv_ace_switch_to(struct kvm_vcpu_arch *vcpu_arch, long fid, long arg0, long arg1);
 
 int kvm_riscv_vcpu_set_interrupt(struct kvm_vcpu *vcpu, unsigned int irq);
 int kvm_riscv_vcpu_unset_interrupt(struct kvm_vcpu *vcpu, unsigned int irq);
diff --git a/arch/riscv/include/asm/kvm_vcpu_sbi.h b/arch/riscv/include/asm/kvm_vcpu_sbi.h
index 8425556af7d1..e7fd6c4d0182 100644
--- a/arch/riscv/include/asm/kvm_vcpu_sbi.h
+++ b/arch/riscv/include/asm/kvm_vcpu_sbi.h
@@ -59,5 +59,6 @@ extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_srst;
 extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_hsm;
 extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_experimental;
 extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_vendor;
+extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_ace;
 
 #endif /* __RISCV_KVM_VCPU_SBI_H__ */
diff --git a/arch/riscv/include/asm/sbi.h b/arch/riscv/include/asm/sbi.h
index 945b7be249c1..87871324865b 100644
--- a/arch/riscv/include/asm/sbi.h
+++ b/arch/riscv/include/asm/sbi.h
@@ -30,6 +30,7 @@ enum sbi_ext_id {
 	SBI_EXT_HSM = 0x48534D,
 	SBI_EXT_SRST = 0x53525354,
 	SBI_EXT_PMU = 0x504D55,
+	SBI_EXT_ACE = 0x509999,
 
 	/* Experimentals extensions must lie within this range */
 	SBI_EXT_EXPERIMENTAL_START = 0x08000000,
diff --git a/arch/riscv/kernel/asm-offsets.c b/arch/riscv/kernel/asm-offsets.c
index df9444397908..153bc32b3072 100644
--- a/arch/riscv/kernel/asm-offsets.c
+++ b/arch/riscv/kernel/asm-offsets.c
@@ -152,6 +152,7 @@ void asm_offsets(void)
 	OFFSET(KVM_ARCH_GUEST_SSTATUS, kvm_vcpu_arch, guest_context.sstatus);
 	OFFSET(KVM_ARCH_GUEST_HSTATUS, kvm_vcpu_arch, guest_context.hstatus);
 	OFFSET(KVM_ARCH_GUEST_SCOUNTEREN, kvm_vcpu_arch, guest_csr.scounteren);
+	OFFSET(KVM_ARCH_GUEST_HTINST, kvm_vcpu_arch, guest_csr.htinst);
 
 	OFFSET(KVM_ARCH_HOST_ZERO, kvm_vcpu_arch, host_context.zero);
 	OFFSET(KVM_ARCH_HOST_RA, kvm_vcpu_arch, host_context.ra);
diff --git a/arch/riscv/kvm/Makefile b/arch/riscv/kvm/Makefile
index 278e97c06e0a..56bfdde53fa4 100644
--- a/arch/riscv/kvm/Makefile
+++ b/arch/riscv/kvm/Makefile
@@ -25,4 +25,5 @@ kvm-y += vcpu_sbi_base.o
 kvm-y += vcpu_sbi_replace.o
 kvm-y += vcpu_sbi_hsm.o
 kvm-y += vcpu_timer.o
+kvm-y += vcpu_sbi_ace.o
 kvm-$(CONFIG_RISCV_PMU_SBI) += vcpu_pmu.o vcpu_sbi_pmu.o
diff --git a/arch/riscv/kvm/mmu.c b/arch/riscv/kvm/mmu.c
index 78211aed36fa..73ea8455d9aa 100644
--- a/arch/riscv/kvm/mmu.c
+++ b/arch/riscv/kvm/mmu.c
@@ -627,7 +627,6 @@ int kvm_riscv_gstage_map(struct kvm_vcpu *vcpu,
 	bool logging = (memslot->dirty_bitmap &&
 			!(memslot->flags & KVM_MEM_READONLY)) ? true : false;
 	unsigned long vma_pagesize, mmu_seq;
-
 	mmap_read_lock(current->mm);
 
 	vma = vma_lookup(current->mm, hva);
diff --git a/arch/riscv/kvm/vcpu.c b/arch/riscv/kvm/vcpu.c
index 7d010b0be54e..e4e7ab5f3f2d 100644
--- a/arch/riscv/kvm/vcpu.c
+++ b/arch/riscv/kvm/vcpu.c
@@ -982,10 +982,17 @@ static void kvm_riscv_update_hvip(struct kvm_vcpu *vcpu)
  */
 static void noinstr kvm_riscv_vcpu_enter_exit(struct kvm_vcpu *vcpu)
 {
-	guest_state_enter_irqoff();
-	__kvm_riscv_switch_to(&vcpu->arch);
-	vcpu->arch.last_exit_cpu = vcpu->cpu;
-	guest_state_exit_irqoff();
+	if (vcpu->arch.is_confidential_vm) {
+		guest_state_enter_irqoff();
+		__kvm_riscv_ace_switch_to(&vcpu->arch, 1010, vcpu->arch.confidential_vm_id, vcpu->arch.vcpu_id);
+		vcpu->arch.last_exit_cpu = vcpu->cpu;
+		guest_state_exit_irqoff();
+	} else {
+		guest_state_enter_irqoff();
+		__kvm_riscv_switch_to(&vcpu->arch);
+		vcpu->arch.last_exit_cpu = vcpu->cpu;
+		guest_state_exit_irqoff();
+	}
 }
 
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
diff --git a/arch/riscv/kvm/vcpu_exit.c b/arch/riscv/kvm/vcpu_exit.c
index 4ea101a73d8b..35811202f271 100644
--- a/arch/riscv/kvm/vcpu_exit.c
+++ b/arch/riscv/kvm/vcpu_exit.c
@@ -23,6 +23,7 @@ static int gstage_page_fault(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	gfn = fault_addr >> PAGE_SHIFT;
 	memslot = gfn_to_memslot(vcpu->kvm, gfn);
 	hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+	// kvm_err("gstage_page_fault A fault_addr=%lx hva=%lx writable=%d\n", fault_addr, hva, writable);
 
 	if (kvm_is_error_hva(hva) ||
 	    (trap->scause == EXC_STORE_GUEST_PAGE_FAULT && !writable)) {
@@ -65,6 +66,8 @@ unsigned long kvm_riscv_vcpu_unpriv_read(struct kvm_vcpu *vcpu,
 	register unsigned long ttmp asm("a1");
 	unsigned long flags, val, tmp, old_stvec, old_hstatus;
 
+	// printk(KERN_INFO "kvm_riscv_vcpu_unpriv_read: %lx %lx\n", taddr, ttmp);
+
 	local_irq_save(flags);
 
 	old_hstatus = csr_swap(CSR_HSTATUS, vcpu->arch.guest_context.hstatus);
@@ -175,12 +178,15 @@ int kvm_riscv_vcpu_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	int ret;
 
 	/* If we got host interrupt then do nothing */
-	if (trap->scause & CAUSE_IRQ_FLAG)
+	if (trap->scause & CAUSE_IRQ_FLAG) {
 		return 1;
+	}
 
 	/* Handle guest traps */
 	ret = -EFAULT;
 	run->exit_reason = KVM_EXIT_UNKNOWN;
+	// kvm_err("kvm_riscv_vcpu_exit: cause=%ld\n", trap->scause);
+
 	switch (trap->scause) {
 	case EXC_INST_ILLEGAL:
 		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV) {
@@ -189,18 +195,21 @@ int kvm_riscv_vcpu_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		}
 		break;
 	case EXC_VIRTUAL_INST_FAULT:
-		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV)
+		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV) {
 			ret = kvm_riscv_vcpu_virtual_insn(vcpu, run, trap);
+		}
 		break;
 	case EXC_INST_GUEST_PAGE_FAULT:
 	case EXC_LOAD_GUEST_PAGE_FAULT:
 	case EXC_STORE_GUEST_PAGE_FAULT:
-		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV)
+		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV) {
 			ret = gstage_page_fault(vcpu, run, trap);
+		}
 		break;
 	case EXC_SUPERVISOR_SYSCALL:
-		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV)
+		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV) {
 			ret = kvm_riscv_vcpu_sbi_ecall(vcpu, run);
+		}
 		break;
 	default:
 		break;
diff --git a/arch/riscv/kvm/vcpu_insn.c b/arch/riscv/kvm/vcpu_insn.c
index f689337b78ff..25b4cdc930cb 100644
--- a/arch/riscv/kvm/vcpu_insn.c
+++ b/arch/riscv/kvm/vcpu_insn.c
@@ -106,6 +106,7 @@
 #define RVC_RS2S(insn)		(8 + RV_X(insn, SH_RS2C, 3))
 #define RVC_RS2(insn)		RV_X(insn, SH_RS2C, 5)
 
+
 #define SHIFT_RIGHT(x, y)		\
 	((y) < 0 ? ((x) << -(y)) : ((x) >> (y)))
 
@@ -416,9 +417,16 @@ int kvm_riscv_vcpu_virtual_insn(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	if (unlikely(INSN_IS_16BIT(insn))) {
 		if (insn == 0) {
 			ct = &vcpu->arch.guest_context;
-			insn = kvm_riscv_vcpu_unpriv_read(vcpu, true,
+			if (vcpu->arch.is_confidential_vm) {
+				// this is a hack because we have difficulties setting up htinst
+				// from the M-mode. A workaround is to store htinst in t6.
+				insn = ct->t6;
+			} else {
+				insn = kvm_riscv_vcpu_unpriv_read(vcpu, true,
 							  ct->sepc,
 							  &utrap);
+			}
+
 			if (utrap.scause) {
 				utrap.sepc = ct->sepc;
 				kvm_riscv_vcpu_trap_redirect(vcpu, &utrap);
@@ -459,6 +467,10 @@ int kvm_riscv_vcpu_mmio_load(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	struct kvm_cpu_trap utrap = { 0 };
 	struct kvm_cpu_context *ct = &vcpu->arch.guest_context;
 
+	// if (fault_addr < 0x10000000 || fault_addr > 0x10000003) {
+		// printk(KERN_INFO "KVM load: htinst=%lx, fault_addr=%lx\n", insn, fault_addr);
+	// }
+
 	/* Determine trapped instruction */
 	if (htinst & 0x1) {
 		/*
@@ -472,8 +484,14 @@ int kvm_riscv_vcpu_mmio_load(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		 * Bit[0] == 0 implies trapped instruction value is
 		 * zero or special value.
 		 */
-		insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc,
-						  &utrap);
+		if (vcpu->arch.is_confidential_vm) {
+			// this is a hack because we have difficulties setting up htinst
+			// from the M-mode. A workaround is to store htinst in t6.
+			insn = ct->t6;
+		} else {
+			insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc, &utrap);
+		}
+
 		if (utrap.scause) {
 			/* Redirect trap if we failed to read instruction */
 			utrap.sepc = ct->sepc;
@@ -506,20 +524,20 @@ int kvm_riscv_vcpu_mmio_load(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	} else if ((insn & INSN_MASK_LHU) == INSN_MATCH_LHU) {
 		len = 2;
 #ifdef CONFIG_64BIT
-	} else if ((insn & INSN_MASK_C_LD) == INSN_MATCH_C_LD) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_LD) == INSN_MATCH_C_LD) {
 		len = 8;
 		shift = 8 * (sizeof(ulong) - len);
 		insn = RVC_RS2S(insn) << SH_RD;
-	} else if ((insn & INSN_MASK_C_LDSP) == INSN_MATCH_C_LDSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_LDSP) == INSN_MATCH_C_LDSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 8;
 		shift = 8 * (sizeof(ulong) - len);
 #endif
-	} else if ((insn & INSN_MASK_C_LW) == INSN_MATCH_C_LW) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_LW) == INSN_MATCH_C_LW) {
 		len = 4;
 		shift = 8 * (sizeof(ulong) - len);
 		insn = RVC_RS2S(insn) << SH_RD;
-	} else if ((insn & INSN_MASK_C_LWSP) == INSN_MATCH_C_LWSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_LWSP) == INSN_MATCH_C_LWSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 4;
 		shift = 8 * (sizeof(ulong) - len);
@@ -598,9 +616,16 @@ int kvm_riscv_vcpu_mmio_store(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		 * Bit[0] == 0 implies trapped instruction value is
 		 * zero or special value.
 		 */
-		insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc,
-						  &utrap);
+		if (vcpu->arch.is_confidential_vm) {
+			// this is a hack because we have difficulties setting up htinst
+			// from the M-mode. A workaround is to store htinst in t6.
+			insn = ct->t6;
+		} else {
+			insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc, &utrap);
+		}
+
 		if (utrap.scause) {
+			// printk(KERN_INFO "KVM store: redirect trap\n");
 			/* Redirect trap if we failed to read instruction */
 			utrap.sepc = ct->sepc;
 			kvm_riscv_vcpu_trap_redirect(vcpu, &utrap);
@@ -609,6 +634,10 @@ int kvm_riscv_vcpu_mmio_store(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		insn_len = INSN_LEN(insn);
 	}
 
+	// if (fault_addr < 0x10000000 || fault_addr > 0x10000003) {
+		// printk(KERN_INFO "KVM store: htinst=%lx, insn=%lx fault_addr=%lx\n", htinst, insn, fault_addr);
+	// }
+
 	data = GET_RS2(insn, &vcpu->arch.guest_context);
 	data8 = data16 = data32 = data64 = data;
 
@@ -623,18 +652,18 @@ int kvm_riscv_vcpu_mmio_store(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	} else if ((insn & INSN_MASK_SH) == INSN_MATCH_SH) {
 		len = 2;
 #ifdef CONFIG_64BIT
-	} else if ((insn & INSN_MASK_C_SD) == INSN_MATCH_C_SD) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_SD) == INSN_MATCH_C_SD) {
 		len = 8;
 		data64 = GET_RS2S(insn, &vcpu->arch.guest_context);
-	} else if ((insn & INSN_MASK_C_SDSP) == INSN_MATCH_C_SDSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_SDSP) == INSN_MATCH_C_SDSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 8;
 		data64 = GET_RS2C(insn, &vcpu->arch.guest_context);
 #endif
-	} else if ((insn & INSN_MASK_C_SW) == INSN_MATCH_C_SW) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_SW) == INSN_MATCH_C_SW) {
 		len = 4;
 		data32 = GET_RS2S(insn, &vcpu->arch.guest_context);
-	} else if ((insn & INSN_MASK_C_SWSP) == INSN_MATCH_C_SWSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_SWSP) == INSN_MATCH_C_SWSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 4;
 		data32 = GET_RS2C(insn, &vcpu->arch.guest_context);
@@ -708,8 +737,10 @@ int kvm_riscv_vcpu_mmio_return(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	ulong insn;
 	int len, shift;
 
-	if (vcpu->arch.mmio_decode.return_handled)
+
+	if (vcpu->arch.mmio_decode.return_handled) {
 		return 0;
+	}
 
 	vcpu->arch.mmio_decode.return_handled = 1;
 	insn = vcpu->arch.mmio_decode.insn;
@@ -723,21 +754,25 @@ int kvm_riscv_vcpu_mmio_return(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	switch (len) {
 	case 1:
 		data8 = *((u8 *)run->mmio.data);
+		// printk(KERN_INFO "KVM MMIO return: %lx to REG_OFFSET(insn, SH_RD)=%d\n", data8, REG_OFFSET(insn, SH_RD));
 		SET_RD(insn, &vcpu->arch.guest_context,
 			(ulong)data8 << shift >> shift);
 		break;
 	case 2:
 		data16 = *((u16 *)run->mmio.data);
+		// printk(KERN_INFO "KVM MMIO return: %lx to REG_OFFSET(insn, SH_RD)=%d\n", data16, REG_OFFSET(insn, SH_RD));
 		SET_RD(insn, &vcpu->arch.guest_context,
 			(ulong)data16 << shift >> shift);
 		break;
 	case 4:
 		data32 = *((u32 *)run->mmio.data);
+		// printk(KERN_INFO "KVM MMIO return: %lx to REG_OFFSET(insn, SH_RD)=%d\n", data32, REG_OFFSET(insn, SH_RD));
 		SET_RD(insn, &vcpu->arch.guest_context,
 			(ulong)data32 << shift >> shift);
 		break;
 	case 8:
 		data64 = *((u64 *)run->mmio.data);
+		// printk(KERN_INFO "KVM MMIO return: %lx to REG_OFFSET(insn, SH_RD)=%d\n", data64, REG_OFFSET(insn, SH_RD));
 		SET_RD(insn, &vcpu->arch.guest_context,
 			(ulong)data64 << shift >> shift);
 		break;
diff --git a/arch/riscv/kvm/vcpu_sbi.c b/arch/riscv/kvm/vcpu_sbi.c
index 15fde15f9fb8..d4c0ab57d6a6 100644
--- a/arch/riscv/kvm/vcpu_sbi.c
+++ b/arch/riscv/kvm/vcpu_sbi.c
@@ -41,6 +41,7 @@ static const struct kvm_vcpu_sbi_extension *sbi_ext[] = {
 	&vcpu_sbi_ext_pmu,
 	&vcpu_sbi_ext_experimental,
 	&vcpu_sbi_ext_vendor,
+	&vcpu_sbi_ext_ace,
 };
 
 void kvm_riscv_vcpu_sbi_forward(struct kvm_vcpu *vcpu, struct kvm_run *run)
@@ -69,6 +70,10 @@ void kvm_riscv_vcpu_sbi_system_reset(struct kvm_vcpu *vcpu,
 	unsigned long i;
 	struct kvm_vcpu *tmp;
 
+	if (vcpu->arch.is_confidential_vm) {
+		vcpu->arch.is_confidential_vm = 0;
+	}
+
 	kvm_for_each_vcpu(i, tmp, vcpu->kvm)
 		tmp->arch.power_off = true;
 	kvm_make_all_cpus_request(vcpu->kvm, KVM_REQ_SLEEP);
@@ -127,6 +132,7 @@ int kvm_riscv_vcpu_sbi_ecall(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	bool ext_is_v01 = false;
 
 	sbi_ext = kvm_vcpu_sbi_find_ext(cp->a7);
+
 	if (sbi_ext && sbi_ext->handler) {
 #ifdef CONFIG_RISCV_SBI_V01
 		if (cp->a7 >= SBI_EXT_0_1_SET_TIMER &&
@@ -151,6 +157,7 @@ int kvm_riscv_vcpu_sbi_ecall(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 	/* Handle special error cases i.e trap, exit or userspace forward */
 	if (sbi_ret.utrap->scause) {
+		printk(KERN_INFO "kvm kvm_riscv_vcpu_sbi_ecall scause!\n");
 		/* No need to increment sepc or exit ioctl loop */
 		ret = 1;
 		sbi_ret.utrap->sepc = cp->sepc;
diff --git a/arch/riscv/kvm/vcpu_sbi_ace.c b/arch/riscv/kvm/vcpu_sbi_ace.c
new file mode 100644
index 000000000000..0e3eee78d537
--- /dev/null
+++ b/arch/riscv/kvm/vcpu_sbi_ace.c
@@ -0,0 +1,133 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2021 IBM.
+ *
+ * Authors:
+ *     Wojciech Ozga <woz@zurich.ibm.com>
+ */
+
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <linux/kvm_host.h>
+#include <asm/sbi.h>
+#include <asm/kvm_vcpu_sbi.h>
+#include <asm/kvm_host.h>
+#include <asm/io.h>
+
+const int SECURITY_MONITOR_EXTID = 0x510000;
+const int SECURITY_MONITOR_PAGE_IN_FID = 2003;
+
+const int SBI_EXT_ACE_LOAD_ALL_PAGES = 0;
+const int SBI_EXT_ACE_REGISTER_SVM = 1;
+const int SBI_EXT_ACE_PAGE_IN = 2;
+
+phys_addr_t test_phys_addr = 0;
+
+static int kvm_sbi_ace_load_all_pages(struct kvm_vcpu *vcpu)
+{
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	struct kvm_memory_slot *memslot;
+	unsigned long hva, fault_addr;
+	bool writable;
+	gfn_t gfn;
+	int ret;
+	long page;
+	unsigned long memory_size = cp->a0;
+
+	printk(KERN_INFO "ACE KVM: Loading all pages\n");
+	unsigned long memory_start = 0x80000000;
+	// unsigned long number_of_pages = memory_size / 4096; 
+	unsigned long number_of_pages = 1024*1024; // ~4GiB because a page is 4KiB;
+	for (page=0; page<number_of_pages; page++) {
+		fault_addr = memory_start + page * 4096;
+		gfn = fault_addr >> PAGE_SHIFT;
+		memslot = gfn_to_memslot(vcpu->kvm, gfn);
+		hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+		if (memslot != NULL && !kvm_is_error_hva(hva)) {
+			kvm_riscv_gstage_map(vcpu, memslot, fault_addr, hva, true);
+		}
+	}
+
+	return 0;
+}
+
+static int kvm_sbi_ace_register_svm(struct kvm_vcpu *vcpu)
+{
+	struct kvm *kvm = vcpu->kvm;
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	unsigned long confidential_vm_id = cp->a0;
+	unsigned long vcpu_id = cp->a1;
+
+	vcpu->arch.is_confidential_vm = 1;
+	vcpu->arch.confidential_vm_id = confidential_vm_id;
+	vcpu->arch.vcpu_id = vcpu_id;
+
+	printk(KERN_INFO "ACE KVM: registered new confidential VM id=%ld\n", confidential_vm_id);
+
+	return 0;
+}
+
+static int kvm_sbi_ace_page_in(struct kvm_vcpu *vcpu, struct kvm_vcpu_sbi_return *retdata)
+{
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	unsigned long confidential_vm_id = vcpu->arch.confidential_vm_id;
+	unsigned long hart_id = vcpu->arch.vcpu_id;
+	unsigned long virt_addr = cp->a0;
+	unsigned long is_error = 0;
+	int page_size = 4096;
+	struct sbiret result;
+	bool writable;
+
+	gpa_t gpa = virt_addr;
+	gfn_t gfn = gpa >> PAGE_SHIFT;
+
+	struct kvm_memory_slot *memslot = gfn_to_memslot(vcpu->kvm, gfn);
+	phys_addr_t hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+	kvm_riscv_gstage_map(vcpu, memslot, gpa, hva, true);
+	unsigned long hfn = gfn_to_pfn_prot(vcpu->kvm, gfn, true, NULL);
+	phys_addr_t hpa = hfn << PAGE_SHIFT;
+
+	struct kvm_cpu_context *reset_cntx = &vcpu->arch.guest_reset_context;
+	retdata->out_val = hpa;
+
+	return 0;
+}
+
+static int kvm_sbi_ext_ace_handler(struct kvm_vcpu *vcpu, struct kvm_run *run,
+				   struct kvm_vcpu_sbi_return *retdata)
+{
+	int ret = 0;
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	struct kvm *kvm = vcpu->kvm;
+	unsigned long funcid = cp->a6;
+
+	switch (funcid) {
+	case SBI_EXT_ACE_LOAD_ALL_PAGES:
+		mutex_lock(&kvm->lock);
+		ret = kvm_sbi_ace_load_all_pages(vcpu);
+		mutex_unlock(&kvm->lock);
+		break;
+	case SBI_EXT_ACE_REGISTER_SVM:
+		mutex_lock(&kvm->lock);
+		ret = kvm_sbi_ace_register_svm(vcpu);
+		mutex_unlock(&kvm->lock);
+		break;
+	case SBI_EXT_ACE_PAGE_IN:
+		mutex_lock(&kvm->lock);
+		ret = kvm_sbi_ace_page_in(vcpu, retdata);
+		mutex_unlock(&kvm->lock);
+		break;
+	default:
+		ret = SBI_ERR_NOT_SUPPORTED;
+	}
+
+	retdata->err_val = ret;
+
+	return 0;
+}
+
+const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_ace = {
+	.extid_start = SBI_EXT_ACE,
+	.extid_end = SBI_EXT_ACE,
+	.handler = kvm_sbi_ext_ace_handler,
+};
diff --git a/arch/riscv/kvm/vcpu_sbi_hsm.c b/arch/riscv/kvm/vcpu_sbi_hsm.c
index 7dca0e9381d9..46f33f29d411 100644
--- a/arch/riscv/kvm/vcpu_sbi_hsm.c
+++ b/arch/riscv/kvm/vcpu_sbi_hsm.c
@@ -32,8 +32,15 @@ static int kvm_sbi_hsm_vcpu_start(struct kvm_vcpu *vcpu)
 	reset_cntx->a0 = target_vcpuid;
 	/* private data passed from kernel */
 	reset_cntx->a1 = cp->a2;
-	kvm_make_request(KVM_REQ_VCPU_RESET, target_vcpu);
 
+	if (vcpu->arch.is_confidential_vm) {
+		printk(KERN_INFO "ACE KVM: starting new vcpu\n");
+		target_vcpu->arch.is_confidential_vm = 1;
+		target_vcpu->arch.confidential_vm_id = vcpu->arch.confidential_vm_id;
+		target_vcpu->arch.vcpu_id = target_vcpuid;
+	}
+
+	kvm_make_request(KVM_REQ_VCPU_RESET, target_vcpu);
 	kvm_riscv_vcpu_power_on(target_vcpu);
 
 	return 0;
diff --git a/arch/riscv/kvm/vcpu_switch.S b/arch/riscv/kvm/vcpu_switch.S
index d74df8eb4d71..d47fe00d11e9 100644
--- a/arch/riscv/kvm/vcpu_switch.S
+++ b/arch/riscv/kvm/vcpu_switch.S
@@ -210,6 +210,216 @@ __kvm_switch_return:
 	ret
 ENDPROC(__kvm_riscv_switch_to)
 
+# ACE START,
+#	a0 - address of the vcpu->arch
+#	a1 - SM-call function ID
+#   a2 - first argument of the SM-call
+ENTRY(__kvm_riscv_ace_switch_to)
+	/* Save Host GPRs (except A0 and T0-T6) */
+	REG_S	ra, (KVM_ARCH_HOST_RA)(a0)
+	REG_S	sp, (KVM_ARCH_HOST_SP)(a0)
+	REG_S	gp, (KVM_ARCH_HOST_GP)(a0)
+	REG_S	tp, (KVM_ARCH_HOST_TP)(a0)
+	REG_S	s0, (KVM_ARCH_HOST_S0)(a0)
+	REG_S	s1, (KVM_ARCH_HOST_S1)(a0)
+	REG_S	a1, (KVM_ARCH_HOST_A1)(a0)
+	REG_S	a2, (KVM_ARCH_HOST_A2)(a0)
+	REG_S	a3, (KVM_ARCH_HOST_A3)(a0)
+	REG_S	a4, (KVM_ARCH_HOST_A4)(a0)
+	REG_S	a5, (KVM_ARCH_HOST_A5)(a0)
+	REG_S	a6, (KVM_ARCH_HOST_A6)(a0)
+	REG_S	a7, (KVM_ARCH_HOST_A7)(a0)
+	REG_S	s2, (KVM_ARCH_HOST_S2)(a0)
+	REG_S	s3, (KVM_ARCH_HOST_S3)(a0)
+	REG_S	s4, (KVM_ARCH_HOST_S4)(a0)
+	REG_S	s5, (KVM_ARCH_HOST_S5)(a0)
+	REG_S	s6, (KVM_ARCH_HOST_S6)(a0)
+	REG_S	s7, (KVM_ARCH_HOST_S7)(a0)
+	REG_S	s8, (KVM_ARCH_HOST_S8)(a0)
+	REG_S	s9, (KVM_ARCH_HOST_S9)(a0)
+	REG_S	s10, (KVM_ARCH_HOST_S10)(a0)
+	REG_S	s11, (KVM_ARCH_HOST_S11)(a0)
+
+	/* Load Guest CSR values */
+	REG_L	t0, (KVM_ARCH_GUEST_SSTATUS)(a0)
+	REG_L	t1, (KVM_ARCH_GUEST_HSTATUS)(a0)
+	REG_L	t2, (KVM_ARCH_GUEST_SCOUNTEREN)(a0)
+	la	t4, __kvm_ace_switch_return
+	REG_L	t5, (KVM_ARCH_GUEST_SEPC)(a0)
+
+	/* Save Host and Restore Guest SSTATUS */
+	csrrw	t0, CSR_SSTATUS, t0
+
+	/* Save Host and Restore Guest HSTATUS */
+	csrrw	t1, CSR_HSTATUS, t1
+
+	/* Save Host and Restore Guest SCOUNTEREN */
+	csrrw	t2, CSR_SCOUNTEREN, t2
+
+	/* Save Host STVEC and change it to return path */
+	csrrw	t4, CSR_STVEC, t4
+
+	/* Save Host SSCRATCH and change it to struct kvm_vcpu_arch pointer */
+	csrrw	t3, CSR_SSCRATCH, a0
+
+	/* Restore Guest SEPC */
+	csrw	CSR_SEPC, t5
+
+	/* Store Host CSR values */
+	REG_S	t0, (KVM_ARCH_HOST_SSTATUS)(a0)
+	REG_S	t1, (KVM_ARCH_HOST_HSTATUS)(a0)
+	REG_S	t2, (KVM_ARCH_HOST_SCOUNTEREN)(a0)
+	REG_S	t3, (KVM_ARCH_HOST_SSCRATCH)(a0)
+	REG_S	t4, (KVM_ARCH_HOST_STVEC)(a0)
+
+	# invoke security monitor resume sm-call
+	li		a7, 0x510000 # ACE_EXT_ID that identifies SM-call
+	add		a6, a1, 0	 # function ID
+	add		t0, a2, 0	 # first argument of the SM-call
+	add		t1, a3, 0	 # 2nd argument of the SM-call
+	add		t2, a4, 0	 # 3rd argument of the SM-call
+	add		t3, a5, 0	 # 4th argument of the SM-call
+
+	/* Restore Guest GPRs (except A0) */
+	REG_L	ra, (KVM_ARCH_GUEST_RA)(a0)
+	REG_L	sp, (KVM_ARCH_GUEST_SP)(a0)
+	REG_L	gp, (KVM_ARCH_GUEST_GP)(a0)
+	REG_L	tp, (KVM_ARCH_GUEST_TP)(a0)
+	# our convention: we use t0-t5 as arguments to ACE
+	# because a0-a5 is used by KVM for hcalls, mmio etc
+	# REG_L	t0, (KVM_ARCH_GUEST_T0)(a0)
+	# REG_L	t1, (KVM_ARCH_GUEST_T1)(a0)
+	# REG_L	t2, (KVM_ARCH_GUEST_T2)(a0)
+	REG_L	s0, (KVM_ARCH_GUEST_S0)(a0)
+	REG_L	s1, (KVM_ARCH_GUEST_S1)(a0)
+	REG_L	a1, (KVM_ARCH_GUEST_A1)(a0)
+	REG_L	a2, (KVM_ARCH_GUEST_A2)(a0)
+	REG_L	a3, (KVM_ARCH_GUEST_A3)(a0)
+	REG_L	a4, (KVM_ARCH_GUEST_A4)(a0)
+	REG_L	a5, (KVM_ARCH_GUEST_A5)(a0)
+	# REG_L	a6, (KVM_ARCH_GUEST_A6)(a0)
+	# REG_L	a7, (KVM_ARCH_GUEST_A7)(a0)
+	REG_L	s2, (KVM_ARCH_GUEST_S2)(a0)
+	REG_L	s3, (KVM_ARCH_GUEST_S3)(a0)
+	REG_L	s4, (KVM_ARCH_GUEST_S4)(a0)
+	REG_L	s5, (KVM_ARCH_GUEST_S5)(a0)
+	REG_L	s6, (KVM_ARCH_GUEST_S6)(a0)
+	REG_L	s7, (KVM_ARCH_GUEST_S7)(a0)
+	REG_L	s8, (KVM_ARCH_GUEST_S8)(a0)
+	REG_L	s9, (KVM_ARCH_GUEST_S9)(a0)
+	REG_L	s10, (KVM_ARCH_GUEST_S10)(a0)
+	REG_L	s11, (KVM_ARCH_GUEST_S11)(a0)
+	# REG_L	t3, (KVM_ARCH_GUEST_T3)(a0)
+	# REG_L	t4, (KVM_ARCH_GUEST_T4)(a0)
+	# REG_L	t5, (KVM_ARCH_GUEST_T5)(a0)
+	# REG_L	t6, (KVM_ARCH_GUEST_T6)(a0)
+
+	REG_L	a0, (KVM_ARCH_GUEST_A0)(a0)
+
+	/* Resume Guest */
+	ecall
+
+	/* Back to Host */
+	.align 4
+__kvm_ace_switch_return:
+	/* Swap Guest A0 with SSCRATCH */
+	csrrw	a0, CSR_SSCRATCH, a0
+
+	/* Save Guest GPRs (except A0) */
+	REG_S	ra, (KVM_ARCH_GUEST_RA)(a0)
+	REG_S	sp, (KVM_ARCH_GUEST_SP)(a0)
+	REG_S	gp, (KVM_ARCH_GUEST_GP)(a0)
+	REG_S	tp, (KVM_ARCH_GUEST_TP)(a0)
+	REG_S	t0, (KVM_ARCH_GUEST_T0)(a0)
+	REG_S	t1, (KVM_ARCH_GUEST_T1)(a0)
+	REG_S	t2, (KVM_ARCH_GUEST_T2)(a0)
+	REG_S	s0, (KVM_ARCH_GUEST_S0)(a0)
+	REG_S	s1, (KVM_ARCH_GUEST_S1)(a0)
+	REG_S	a1, (KVM_ARCH_GUEST_A1)(a0)
+	REG_S	a2, (KVM_ARCH_GUEST_A2)(a0)
+	REG_S	a3, (KVM_ARCH_GUEST_A3)(a0)
+	REG_S	a4, (KVM_ARCH_GUEST_A4)(a0)
+	REG_S	a5, (KVM_ARCH_GUEST_A5)(a0)
+	REG_S	a6, (KVM_ARCH_GUEST_A6)(a0)
+	REG_S	a7, (KVM_ARCH_GUEST_A7)(a0)
+	REG_S	s2, (KVM_ARCH_GUEST_S2)(a0)
+	REG_S	s3, (KVM_ARCH_GUEST_S3)(a0)
+	REG_S	s4, (KVM_ARCH_GUEST_S4)(a0)
+	REG_S	s5, (KVM_ARCH_GUEST_S5)(a0)
+	REG_S	s6, (KVM_ARCH_GUEST_S6)(a0)
+	REG_S	s7, (KVM_ARCH_GUEST_S7)(a0)
+	REG_S	s8, (KVM_ARCH_GUEST_S8)(a0)
+	REG_S	s9, (KVM_ARCH_GUEST_S9)(a0)
+	REG_S	s10, (KVM_ARCH_GUEST_S10)(a0)
+	REG_S	s11, (KVM_ARCH_GUEST_S11)(a0)
+	REG_S	t3, (KVM_ARCH_GUEST_T3)(a0)
+	REG_S	t4, (KVM_ARCH_GUEST_T4)(a0)
+	REG_S	t5, (KVM_ARCH_GUEST_T5)(a0)
+	REG_S	t6, (KVM_ARCH_GUEST_T6)(a0)
+
+	/* Load Host CSR values */
+	REG_L	t1, (KVM_ARCH_HOST_STVEC)(a0)
+	REG_L	t2, (KVM_ARCH_HOST_SSCRATCH)(a0)
+	REG_L	t3, (KVM_ARCH_HOST_SCOUNTEREN)(a0)
+	REG_L	t4, (KVM_ARCH_HOST_HSTATUS)(a0)
+	REG_L	t5, (KVM_ARCH_HOST_SSTATUS)(a0)
+
+	/* Save Guest SEPC */
+	csrr	t0, CSR_SEPC
+
+	/* Save Guest A0 and Restore Host SSCRATCH */
+	csrrw	t2, CSR_SSCRATCH, t2
+
+	/* Restore Host STVEC */
+	csrw	CSR_STVEC, t1
+
+	/* Save Guest and Restore Host SCOUNTEREN */
+	csrrw	t3, CSR_SCOUNTEREN, t3
+
+	/* Save Guest and Restore Host HSTATUS */
+	csrrw	t4, CSR_HSTATUS, t4
+
+	/* Save Guest and Restore Host SSTATUS */
+	csrrw	t5, CSR_SSTATUS, t5
+
+	/* Store Guest CSR values */
+	REG_S	t0, (KVM_ARCH_GUEST_SEPC)(a0)
+	REG_S	t2, (KVM_ARCH_GUEST_A0)(a0)
+	REG_S	t3, (KVM_ARCH_GUEST_SCOUNTEREN)(a0)
+	REG_S	t4, (KVM_ARCH_GUEST_HSTATUS)(a0)
+	REG_S	t5, (KVM_ARCH_GUEST_SSTATUS)(a0)
+
+	/* Restore Host GPRs (except A0 and T0-T6) */
+	REG_L	ra, (KVM_ARCH_HOST_RA)(a0)
+	REG_L	sp, (KVM_ARCH_HOST_SP)(a0)
+	REG_L	gp, (KVM_ARCH_HOST_GP)(a0)
+	REG_L	tp, (KVM_ARCH_HOST_TP)(a0)
+	REG_L	s0, (KVM_ARCH_HOST_S0)(a0)
+	REG_L	s1, (KVM_ARCH_HOST_S1)(a0)
+	REG_L	a1, (KVM_ARCH_HOST_A1)(a0)
+	REG_L	a2, (KVM_ARCH_HOST_A2)(a0)
+	REG_L	a3, (KVM_ARCH_HOST_A3)(a0)
+	REG_L	a4, (KVM_ARCH_HOST_A4)(a0)
+	REG_L	a5, (KVM_ARCH_HOST_A5)(a0)
+	REG_L	a6, (KVM_ARCH_HOST_A6)(a0)
+	REG_L	a7, (KVM_ARCH_HOST_A7)(a0)
+	REG_L	s2, (KVM_ARCH_HOST_S2)(a0)
+	REG_L	s3, (KVM_ARCH_HOST_S3)(a0)
+	REG_L	s4, (KVM_ARCH_HOST_S4)(a0)
+	REG_L	s5, (KVM_ARCH_HOST_S5)(a0)
+	REG_L	s6, (KVM_ARCH_HOST_S6)(a0)
+	REG_L	s7, (KVM_ARCH_HOST_S7)(a0)
+	REG_L	s8, (KVM_ARCH_HOST_S8)(a0)
+	REG_L	s9, (KVM_ARCH_HOST_S9)(a0)
+	REG_L	s10, (KVM_ARCH_HOST_S10)(a0)
+	REG_L	s11, (KVM_ARCH_HOST_S11)(a0)
+
+	/* Return to C code */
+	ret
+ENDPROC(__kvm_riscv_ace_switch_to)
+
+# ACE END
+
 ENTRY(__kvm_riscv_unpriv_trap)
 	/*
 	 * We assume that faulting unpriv load/store instruction is
diff --git a/rust/alloc/alloc.rs b/rust/alloc/alloc.rs
index ca224a541770..65ce3fea232a 100644
--- a/rust/alloc/alloc.rs
+++ b/rust/alloc/alloc.rs
@@ -31,13 +31,13 @@ extern "Rust" {
     // like `malloc`, `realloc`, and `free`, respectively.
     #[rustc_allocator]
     #[rustc_allocator_nounwind]
-    fn __rust_alloc(size: usize, align: usize) -> *mut u8;
+    fn __rust_alloc(size: usize, align: usize) -> *mut usize;
     #[rustc_allocator_nounwind]
-    fn __rust_dealloc(ptr: *mut u8, size: usize, align: usize);
+    fn __rust_dealloc(ptr: *mut usize, size: usize, align: usize);
     #[rustc_allocator_nounwind]
-    fn __rust_realloc(ptr: *mut u8, old_size: usize, align: usize, new_size: usize) -> *mut u8;
+    fn __rust_realloc(ptr: *mut usize, old_size: usize, align: usize, new_size: usize) -> *mut usize;
     #[rustc_allocator_nounwind]
-    fn __rust_alloc_zeroed(size: usize, align: usize) -> *mut u8;
+    fn __rust_alloc_zeroed(size: usize, align: usize) -> *mut usize;
 }
 
 /// The global memory allocator.
@@ -87,7 +87,7 @@ pub use std::alloc::Global;
 #[stable(feature = "global_alloc", since = "1.28.0")]
 #[must_use = "losing the pointer will leak memory"]
 #[inline]
-pub unsafe fn alloc(layout: Layout) -> *mut u8 {
+pub unsafe fn alloc(layout: Layout) -> *mut usize {
     unsafe { __rust_alloc(layout.size(), layout.align()) }
 }
 
@@ -105,7 +105,7 @@ pub unsafe fn alloc(layout: Layout) -> *mut u8 {
 /// See [`GlobalAlloc::dealloc`].
 #[stable(feature = "global_alloc", since = "1.28.0")]
 #[inline]
-pub unsafe fn dealloc(ptr: *mut u8, layout: Layout) {
+pub unsafe fn dealloc(ptr: *mut usize, layout: Layout) {
     unsafe { __rust_dealloc(ptr, layout.size(), layout.align()) }
 }
 
@@ -124,7 +124,7 @@ pub unsafe fn dealloc(ptr: *mut u8, layout: Layout) {
 #[stable(feature = "global_alloc", since = "1.28.0")]
 #[must_use = "losing the pointer will leak memory"]
 #[inline]
-pub unsafe fn realloc(ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {
+pub unsafe fn realloc(ptr: *mut usize, layout: Layout, new_size: usize) -> *mut usize {
     unsafe { __rust_realloc(ptr, layout.size(), layout.align(), new_size) }
 }
 
@@ -158,7 +158,7 @@ pub unsafe fn realloc(ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8
 #[stable(feature = "global_alloc", since = "1.28.0")]
 #[must_use = "losing the pointer will leak memory"]
 #[inline]
-pub unsafe fn alloc_zeroed(layout: Layout) -> *mut u8 {
+pub unsafe fn alloc_zeroed(layout: Layout) -> *mut usize {
     unsafe { __rust_alloc_zeroed(layout.size(), layout.align()) }
 }
 
@@ -317,7 +317,7 @@ unsafe impl Allocator for Global {
 #[cfg(all(not(no_global_oom_handling), not(test)))]
 #[lang = "exchange_malloc"]
 #[inline]
-unsafe fn exchange_malloc(size: usize, align: usize) -> *mut u8 {
+unsafe fn exchange_malloc(size: usize, align: usize) -> *mut usize {
     let layout = unsafe { Layout::from_size_align_unchecked(size, align) };
     match Global.allocate(layout) {
         Ok(ptr) => ptr.as_mut_ptr(),
diff --git a/rust/alloc/boxed.rs b/rust/alloc/boxed.rs
index dcfe87b14f3a..309506c36080 100644
--- a/rust/alloc/boxed.rs
+++ b/rust/alloc/boxed.rs
@@ -1020,7 +1020,7 @@ impl<T: ?Sized, A: Allocator> Box<T, A> {
     /// let p = Box::into_raw(x);
     /// unsafe {
     ///     ptr::drop_in_place(p);
-    ///     dealloc(p as *mut u8, Layout::new::<String>());
+    ///     dealloc(p as *mut usize, Layout::new::<String>());
     /// }
     /// ```
     ///
diff --git a/rust/alloc/vec/mod.rs b/rust/alloc/vec/mod.rs
index f77c7368d534..c7d756a82276 100644
--- a/rust/alloc/vec/mod.rs
+++ b/rust/alloc/vec/mod.rs
@@ -1313,7 +1313,7 @@ impl<T, A: Allocator> Vec<T, A> {
     /// # extern "C" {
     /// #     fn deflateGetDictionary(
     /// #         strm: *mut std::ffi::c_void,
-    /// #         dictionary: *mut u8,
+    /// #         dictionary: *mut usize,
     /// #         dictLength: *mut usize,
     /// #     ) -> i32;
     /// # }
diff --git a/rust/kernel/allocator.rs b/rust/kernel/allocator.rs
index 397a3dd57a9b..f3cea1551117 100644
--- a/rust/kernel/allocator.rs
+++ b/rust/kernel/allocator.rs
@@ -10,13 +10,13 @@ use crate::bindings;
 struct KernelAllocator;
 
 unsafe impl GlobalAlloc for KernelAllocator {
-    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
+    unsafe fn alloc(&self, layout: Layout) -> *mut usize {
         // `krealloc()` is used instead of `kmalloc()` because the latter is
         // an inline function and cannot be bound to as a result.
-        unsafe { bindings::krealloc(ptr::null(), layout.size(), bindings::GFP_KERNEL) as *mut u8 }
+        unsafe { bindings::krealloc(ptr::null(), layout.size(), bindings::GFP_KERNEL) as *mut usize }
     }
 
-    unsafe fn dealloc(&self, ptr: *mut u8, _layout: Layout) {
+    unsafe fn dealloc(&self, ptr: *mut usize, _layout: Layout) {
         unsafe {
             bindings::kfree(ptr as *const core::ffi::c_void);
         }
@@ -32,33 +32,33 @@ static ALLOCATOR: KernelAllocator = KernelAllocator;
 //
 // Note that `#[no_mangle]` implies exported too, nowadays.
 #[no_mangle]
-fn __rust_alloc(size: usize, _align: usize) -> *mut u8 {
-    unsafe { bindings::krealloc(core::ptr::null(), size, bindings::GFP_KERNEL) as *mut u8 }
+fn __rust_alloc(size: usize, _align: usize) -> *mut usize {
+    unsafe { bindings::krealloc(core::ptr::null(), size, bindings::GFP_KERNEL) as *mut usize }
 }
 
 #[no_mangle]
-fn __rust_dealloc(ptr: *mut u8, _size: usize, _align: usize) {
+fn __rust_dealloc(ptr: *mut usize, _size: usize, _align: usize) {
     unsafe { bindings::kfree(ptr as *const core::ffi::c_void) };
 }
 
 #[no_mangle]
-fn __rust_realloc(ptr: *mut u8, _old_size: usize, _align: usize, new_size: usize) -> *mut u8 {
+fn __rust_realloc(ptr: *mut usize, _old_size: usize, _align: usize, new_size: usize) -> *mut usize {
     unsafe {
         bindings::krealloc(
             ptr as *const core::ffi::c_void,
             new_size,
             bindings::GFP_KERNEL,
-        ) as *mut u8
+        ) as *mut usize
     }
 }
 
 #[no_mangle]
-fn __rust_alloc_zeroed(size: usize, _align: usize) -> *mut u8 {
+fn __rust_alloc_zeroed(size: usize, _align: usize) -> *mut usize {
     unsafe {
         bindings::krealloc(
             core::ptr::null(),
             size,
             bindings::GFP_KERNEL | bindings::__GFP_ZERO,
-        ) as *mut u8
+        ) as *mut usize
     }
 }
diff --git a/rust/kernel/str.rs b/rust/kernel/str.rs
index b771310fa4a4..f3e0c2035f86 100644
--- a/rust/kernel/str.rs
+++ b/rust/kernel/str.rs
@@ -407,7 +407,7 @@ impl RawFormatter {
     ///
     /// If `pos` is less than `end`, then the region between `pos` (inclusive) and `end`
     /// (exclusive) must be valid for writes for the lifetime of the returned [`RawFormatter`].
-    pub(crate) unsafe fn from_ptrs(pos: *mut u8, end: *mut u8) -> Self {
+    pub(crate) unsafe fn from_ptrs(pos: *mut usize, end: *mut usize) -> Self {
         // INVARIANT: The safety requierments guarantee the type invariants.
         Self {
             beg: pos as _,
@@ -422,7 +422,7 @@ impl RawFormatter {
     ///
     /// The memory region starting at `buf` and extending for `len` bytes must be valid for writes
     /// for the lifetime of the returned [`RawFormatter`].
-    pub(crate) unsafe fn from_buffer(buf: *mut u8, len: usize) -> Self {
+    pub(crate) unsafe fn from_buffer(buf: *mut usize, len: usize) -> Self {
         let pos = buf as usize;
         // INVARIANT: We ensure that `end` is never less then `buf`, and the safety requirements
         // guarantees that the memory region is valid for writes.
@@ -436,7 +436,7 @@ impl RawFormatter {
     /// Returns the current insert position.
     ///
     /// N.B. It may point to invalid memory.
-    pub(crate) fn pos(&self) -> *mut u8 {
+    pub(crate) fn pos(&self) -> *mut usize {
         self.pos as _
     }
 
@@ -461,7 +461,7 @@ impl fmt::Write for RawFormatter {
             unsafe {
                 core::ptr::copy_nonoverlapping(
                     s.as_bytes().as_ptr(),
-                    self.pos as *mut u8,
+                    self.pos as *mut usize,
                     len_to_copy,
                 )
             };
@@ -484,7 +484,7 @@ impl Formatter {
     ///
     /// The memory region starting at `buf` and extending for `len` bytes must be valid for writes
     /// for the lifetime of the returned [`Formatter`].
-    pub(crate) unsafe fn from_buffer(buf: *mut u8, len: usize) -> Self {
+    pub(crate) unsafe fn from_buffer(buf: *mut usize, len: usize) -> Self {
         // SAFETY: The safety requirements of this function satisfy those of the callee.
         Self(unsafe { RawFormatter::from_buffer(buf, len) })
     }
