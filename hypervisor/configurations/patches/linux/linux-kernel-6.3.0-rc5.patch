diff --git a/arch/riscv/include/asm/kvm_host.h b/arch/riscv/include/asm/kvm_host.h
index cc7da66ee0c0..901ea5be5bc4 100644
--- a/arch/riscv/include/asm/kvm_host.h
+++ b/arch/riscv/include/asm/kvm_host.h
@@ -154,6 +154,7 @@ struct kvm_vcpu_csr {
 	unsigned long hvip;
 	unsigned long vsatp;
 	unsigned long scounteren;
+	unsigned long htinst;
 };
 
 struct kvm_vcpu_arch {
@@ -232,6 +233,12 @@ struct kvm_vcpu_arch {
 
 	/* Performance monitoring context */
 	struct kvm_pmu pmu_context;
+
+	// ACE START
+	bool is_svm;
+	unsigned long svm_id;
+	unsigned long vcpu_id;
+	// ACE END
 };
 
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
@@ -322,6 +329,7 @@ int kvm_riscv_vcpu_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 			struct kvm_cpu_trap *trap);
 
 void __kvm_riscv_switch_to(struct kvm_vcpu_arch *vcpu_arch);
+void __kvm_riscv_ace_switch_to(struct kvm_vcpu_arch *vcpu_arch, long fid, long arg0, long arg1);
 
 int kvm_riscv_vcpu_set_interrupt(struct kvm_vcpu *vcpu, unsigned int irq);
 int kvm_riscv_vcpu_unset_interrupt(struct kvm_vcpu *vcpu, unsigned int irq);
diff --git a/arch/riscv/include/asm/kvm_vcpu_sbi.h b/arch/riscv/include/asm/kvm_vcpu_sbi.h
index 8425556af7d1..d6d81aa6143d 100644
--- a/arch/riscv/include/asm/kvm_vcpu_sbi.h
+++ b/arch/riscv/include/asm/kvm_vcpu_sbi.h
@@ -59,5 +59,6 @@ extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_srst;
 extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_hsm;
 extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_experimental;
 extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_vendor;
+extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_ace;
 
 #endif /* __RISCV_KVM_VCPU_SBI_H__ */
diff --git a/arch/riscv/include/asm/sbi.h b/arch/riscv/include/asm/sbi.h
index 945b7be249c1..254fe69becf3 100644
--- a/arch/riscv/include/asm/sbi.h
+++ b/arch/riscv/include/asm/sbi.h
@@ -30,6 +30,7 @@ enum sbi_ext_id {
 	SBI_EXT_HSM = 0x48534D,
 	SBI_EXT_SRST = 0x53525354,
 	SBI_EXT_PMU = 0x504D55,
+	SBI_EXT_ACE = 0x509999,
 
 	/* Experimentals extensions must lie within this range */
 	SBI_EXT_EXPERIMENTAL_START = 0x08000000,
diff --git a/arch/riscv/kernel/asm-offsets.c b/arch/riscv/kernel/asm-offsets.c
index df9444397908..153bc32b3072 100644
--- a/arch/riscv/kernel/asm-offsets.c
+++ b/arch/riscv/kernel/asm-offsets.c
@@ -152,6 +152,7 @@ void asm_offsets(void)
 	OFFSET(KVM_ARCH_GUEST_SSTATUS, kvm_vcpu_arch, guest_context.sstatus);
 	OFFSET(KVM_ARCH_GUEST_HSTATUS, kvm_vcpu_arch, guest_context.hstatus);
 	OFFSET(KVM_ARCH_GUEST_SCOUNTEREN, kvm_vcpu_arch, guest_csr.scounteren);
+	OFFSET(KVM_ARCH_GUEST_HTINST, kvm_vcpu_arch, guest_csr.htinst);
 
 	OFFSET(KVM_ARCH_HOST_ZERO, kvm_vcpu_arch, host_context.zero);
 	OFFSET(KVM_ARCH_HOST_RA, kvm_vcpu_arch, host_context.ra);
diff --git a/arch/riscv/kvm/Makefile b/arch/riscv/kvm/Makefile
index 278e97c06e0a..34846bc51877 100644
--- a/arch/riscv/kvm/Makefile
+++ b/arch/riscv/kvm/Makefile
@@ -25,4 +25,5 @@ kvm-y += vcpu_sbi_base.o
 kvm-y += vcpu_sbi_replace.o
 kvm-y += vcpu_sbi_hsm.o
 kvm-y += vcpu_timer.o
+kvm-y += vcpu_sbi_ace.o
 kvm-$(CONFIG_RISCV_PMU_SBI) += vcpu_pmu.o vcpu_sbi_pmu.o
diff --git a/arch/riscv/kvm/mmu.c b/arch/riscv/kvm/mmu.c
index 78211aed36fa..5860fc82611a 100644
--- a/arch/riscv/kvm/mmu.c
+++ b/arch/riscv/kvm/mmu.c
@@ -480,6 +480,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 
 	mmap_read_lock(current->mm);
 
+
 	/*
 	 * A memory region could potentially cover multiple VMAs, and
 	 * any holes between them, so iterate over all of them to find
@@ -627,7 +628,6 @@ int kvm_riscv_gstage_map(struct kvm_vcpu *vcpu,
 	bool logging = (memslot->dirty_bitmap &&
 			!(memslot->flags & KVM_MEM_READONLY)) ? true : false;
 	unsigned long vma_pagesize, mmu_seq;
-
 	mmap_read_lock(current->mm);
 
 	vma = vma_lookup(current->mm, hva);
diff --git a/arch/riscv/kvm/vcpu.c b/arch/riscv/kvm/vcpu.c
index 7d010b0be54e..6e24dcbeb575 100644
--- a/arch/riscv/kvm/vcpu.c
+++ b/arch/riscv/kvm/vcpu.c
@@ -743,6 +743,8 @@ void kvm_riscv_vcpu_sync_interrupts(struct kvm_vcpu *vcpu)
 	struct kvm_vcpu_arch *v = &vcpu->arch;
 	struct kvm_vcpu_csr *csr = &vcpu->arch.guest_csr;
 
+	// printk(KERN_INFO "kvm_riscv_vcpu_sync_interrupts\n");
+
 	/* Read current HVIP and VSIE CSRs */
 	csr->vsie = csr_read(CSR_VSIE);
 
@@ -982,10 +984,19 @@ static void kvm_riscv_update_hvip(struct kvm_vcpu *vcpu)
  */
 static void noinstr kvm_riscv_vcpu_enter_exit(struct kvm_vcpu *vcpu)
 {
-	guest_state_enter_irqoff();
-	__kvm_riscv_switch_to(&vcpu->arch);
-	vcpu->arch.last_exit_cpu = vcpu->cpu;
-	guest_state_exit_irqoff();
+	if (vcpu->arch.is_svm) {		
+		guest_state_enter_irqoff();
+		// 1010 is the resume SM-call
+		__kvm_riscv_ace_switch_to(&vcpu->arch, 1010, vcpu->arch.svm_id, vcpu->arch.vcpu_id);
+		vcpu->arch.last_exit_cpu = vcpu->cpu;
+		guest_state_exit_irqoff();
+	} else {
+		guest_state_enter_irqoff();
+		//printk(KERN_INFO "return %x %x %x\n", vcpu->arch.guest_context.sepc, vcpu->arch.guest_context.sstatus, vcpu->arch.guest_context.hstatus);
+		__kvm_riscv_switch_to(&vcpu->arch);		
+		vcpu->arch.last_exit_cpu = vcpu->cpu;
+		guest_state_exit_irqoff();
+	}
 }
 
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
@@ -999,6 +1010,8 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 
 	kvm_vcpu_srcu_read_lock(vcpu);
 
+	// printk(KERN_INFO "ACE KVM: kvm_arch_vcpu_ioctl_run: %d\n", run->exit_reason);
+
 	switch (run->exit_reason) {
 	case KVM_EXIT_MMIO:
 		/* Process MMIO value returned from user-space */
@@ -1101,6 +1114,8 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		trap.htval = csr_read(CSR_HTVAL);
 		trap.htinst = csr_read(CSR_HTINST);
 
+		// kvm_err("kvm_arch_vcpu_ioctl_run: %lx\n", trap.htinst);
+
 		/* Syncup interrupts state with HW */
 		kvm_riscv_vcpu_sync_interrupts(vcpu);
 
diff --git a/arch/riscv/kvm/vcpu_exit.c b/arch/riscv/kvm/vcpu_exit.c
index 4ea101a73d8b..5a4eab6e2f23 100644
--- a/arch/riscv/kvm/vcpu_exit.c
+++ b/arch/riscv/kvm/vcpu_exit.c
@@ -24,6 +24,8 @@ static int gstage_page_fault(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	memslot = gfn_to_memslot(vcpu->kvm, gfn);
 	hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
 
+	// kvm_err("gstage_page_fault A fault_addr=%lx hva=%lx writable=%d\n", fault_addr, hva, writable);
+
 	if (kvm_is_error_hva(hva) ||
 	    (trap->scause == EXC_STORE_GUEST_PAGE_FAULT && !writable)) {
 		switch (trap->scause) {
@@ -40,6 +42,8 @@ static int gstage_page_fault(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		};
 	}
 
+	// kvm_err("gstage_page_fault B kvm_riscv_gstage_map %lx %lx %lx %lx\n", fault_addr, hva, trap->htval, trap->stval);
+
 	ret = kvm_riscv_gstage_map(vcpu, memslot, fault_addr, hva,
 		(trap->scause == EXC_STORE_GUEST_PAGE_FAULT) ? true : false);
 	if (ret < 0)
@@ -172,35 +176,60 @@ void kvm_riscv_vcpu_trap_redirect(struct kvm_vcpu *vcpu,
 int kvm_riscv_vcpu_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 			struct kvm_cpu_trap *trap)
 {
-	int ret;
+	int ret;	
 
 	/* If we got host interrupt then do nothing */
-	if (trap->scause & CAUSE_IRQ_FLAG)
+	if (trap->scause & CAUSE_IRQ_FLAG) {
 		return 1;
+	}
 
 	/* Handle guest traps */
 	ret = -EFAULT;
 	run->exit_reason = KVM_EXIT_UNKNOWN;
+
+	// kvm_err("kvm_riscv_vcpu_exit: cause=%ld\n", trap->scause);
+
 	switch (trap->scause) {
 	case EXC_INST_ILLEGAL:
 		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV) {
+			kvm_err("kvm_riscv_vcpu_exit: enter EXC_INST_ILLEGAL\n");
 			kvm_riscv_vcpu_trap_redirect(vcpu, trap);
 			ret = 1;
 		}
 		break;
 	case EXC_VIRTUAL_INST_FAULT:
-		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV)
+		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV) {
+			kvm_err("kvm_riscv_vcpu_exit: enter EXC_VIRTUAL_INST_FAULT\n");
 			ret = kvm_riscv_vcpu_virtual_insn(vcpu, run, trap);
+		}
 		break;
 	case EXC_INST_GUEST_PAGE_FAULT:
 	case EXC_LOAD_GUEST_PAGE_FAULT:
 	case EXC_STORE_GUEST_PAGE_FAULT:
-		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV)
+		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV) {
+			// kvm_err("SEPC=0x%lx SSTATUS=0x%lx HSTATUS=0x%lx\n", vcpu->arch.guest_context.sepc, vcpu->arch.guest_context.sstatus, vcpu->arch.guest_context.hstatus);
+			// kvm_err("hvip=0x%lx scounteren=0x%lx htinst=0x%lx\n", vcpu->arch.guest_csr.hvip, vcpu->arch.guest_csr.scounteren, vcpu->arch.guest_csr.htinst);
+			// kvm_err("SCAUSE=0x%lx STVAL=0x%lx HTVAL=0x%lx HTINST=0x%lx\n", trap->scause, trap->stval, trap->htval, trap->htinst);
+			// kvm_err("0=0x%lx 1=0x%lx 2=0x%lx\n", vcpu->arch.guest_context.zero, vcpu->arch.guest_context.ra, vcpu->arch.guest_context.sp);
+			// kvm_err("3=0x%lx 4=0x%lx 5=0x%lx\n", vcpu->arch.guest_context.gp, vcpu->arch.guest_context.tp, vcpu->arch.guest_context.t0);
+			// kvm_err("6=0x%lx 7=0x%lx 8=0x%lx\n", vcpu->arch.guest_context.t1, vcpu->arch.guest_context.t2, vcpu->arch.guest_context.s0);
+			// kvm_err("9=0x%lx 10=0x%lx 11=0x%lx\n", vcpu->arch.guest_context.s1, vcpu->arch.guest_context.a0, vcpu->arch.guest_context.a1);
+			// kvm_err("12=0x%lx 13=0x%lx 14=0x%lx\n", vcpu->arch.guest_context.a2, vcpu->arch.guest_context.a3, vcpu->arch.guest_context.a4);
+			// kvm_err("15=0x%lx 16=0x%lx 17=0x%lx\n", vcpu->arch.guest_context.a5, vcpu->arch.guest_context.a6, vcpu->arch.guest_context.a7);
+			// kvm_err("18=0x%lx 19=0x%lx 20=0x%lx\n", vcpu->arch.guest_context.s2, vcpu->arch.guest_context.s3, vcpu->arch.guest_context.s4);
+			// kvm_err("21=0x%lx 22=0x%lx 23=0x%lx\n", vcpu->arch.guest_context.s5, vcpu->arch.guest_context.s6, vcpu->arch.guest_context.s7);
+			// kvm_err("24=0x%lx 25=0x%lx 26=0x%lx\n", vcpu->arch.guest_context.s8, vcpu->arch.guest_context.s9, vcpu->arch.guest_context.s10);
+			// kvm_err("27=0x%lx 28=0x%lx 29=0x%lx\n", vcpu->arch.guest_context.s11, vcpu->arch.guest_context.t3, vcpu->arch.guest_context.t4);
+			// kvm_err("30=0x%lx 31=0x%lx\n", vcpu->arch.guest_context.t5, vcpu->arch.guest_context.t6);			
+			// kvm_err("kvm_riscv_vcpu_exit: enter EXC_STORE_GUEST_PAGE_FAULT\n");
 			ret = gstage_page_fault(vcpu, run, trap);
+		}
 		break;
 	case EXC_SUPERVISOR_SYSCALL:
-		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV)
+		if (vcpu->arch.guest_context.hstatus & HSTATUS_SPV) {		
+			// kvm_err("kvm_riscv_vcpu_exit: enter EXC_SUPERVISOR_SYSCALL\n");
 			ret = kvm_riscv_vcpu_sbi_ecall(vcpu, run);
+		}
 		break;
 	default:
 		break;
diff --git a/arch/riscv/kvm/vcpu_insn.c b/arch/riscv/kvm/vcpu_insn.c
index f689337b78ff..a66416983df3 100644
--- a/arch/riscv/kvm/vcpu_insn.c
+++ b/arch/riscv/kvm/vcpu_insn.c
@@ -106,6 +106,7 @@
 #define RVC_RS2S(insn)		(8 + RV_X(insn, SH_RS2C, 3))
 #define RVC_RS2(insn)		RV_X(insn, SH_RS2C, 5)
 
+
 #define SHIFT_RIGHT(x, y)		\
 	((y) < 0 ? ((x) << -(y)) : ((x) >> (y)))
 
@@ -416,9 +417,16 @@ int kvm_riscv_vcpu_virtual_insn(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	if (unlikely(INSN_IS_16BIT(insn))) {
 		if (insn == 0) {
 			ct = &vcpu->arch.guest_context;
-			insn = kvm_riscv_vcpu_unpriv_read(vcpu, true,
+			if (vcpu->arch.is_svm) {
+				// this is a hack because we have difficulties setting up htinst
+				// from the M-mode. A workaround is to store htinst in t6.
+				insn = ct->t6;
+			} else {
+				insn = kvm_riscv_vcpu_unpriv_read(vcpu, true,
 							  ct->sepc,
 							  &utrap);
+			}
+			
 			if (utrap.scause) {
 				utrap.sepc = ct->sepc;
 				kvm_riscv_vcpu_trap_redirect(vcpu, &utrap);
@@ -459,6 +467,10 @@ int kvm_riscv_vcpu_mmio_load(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	struct kvm_cpu_trap utrap = { 0 };
 	struct kvm_cpu_context *ct = &vcpu->arch.guest_context;
 
+	// if (fault_addr < 0x10000000 || fault_addr > 0x10000003) {
+		// printk(KERN_INFO "KVM load: htinst=%lx, fault_addr=%lx\n", insn, fault_addr);
+	// }
+
 	/* Determine trapped instruction */
 	if (htinst & 0x1) {
 		/*
@@ -472,8 +484,14 @@ int kvm_riscv_vcpu_mmio_load(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		 * Bit[0] == 0 implies trapped instruction value is
 		 * zero or special value.
 		 */
-		insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc,
-						  &utrap);
+		if (vcpu->arch.is_svm) {
+			// this is a hack because we have difficulties setting up htinst
+			// from the M-mode. A workaround is to store htinst in t6.
+			insn = ct->t6;
+		} else {
+			insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc, &utrap);
+		}
+		
 		if (utrap.scause) {
 			/* Redirect trap if we failed to read instruction */
 			utrap.sepc = ct->sepc;
@@ -506,20 +524,20 @@ int kvm_riscv_vcpu_mmio_load(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	} else if ((insn & INSN_MASK_LHU) == INSN_MATCH_LHU) {
 		len = 2;
 #ifdef CONFIG_64BIT
-	} else if ((insn & INSN_MASK_C_LD) == INSN_MATCH_C_LD) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_LD) == INSN_MATCH_C_LD) {
 		len = 8;
 		shift = 8 * (sizeof(ulong) - len);
 		insn = RVC_RS2S(insn) << SH_RD;
-	} else if ((insn & INSN_MASK_C_LDSP) == INSN_MATCH_C_LDSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_LDSP) == INSN_MATCH_C_LDSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 8;
 		shift = 8 * (sizeof(ulong) - len);
 #endif
-	} else if ((insn & INSN_MASK_C_LW) == INSN_MATCH_C_LW) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_LW) == INSN_MATCH_C_LW) {
 		len = 4;
 		shift = 8 * (sizeof(ulong) - len);
 		insn = RVC_RS2S(insn) << SH_RD;
-	} else if ((insn & INSN_MASK_C_LWSP) == INSN_MATCH_C_LWSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_LWSP) == INSN_MATCH_C_LWSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 4;
 		shift = 8 * (sizeof(ulong) - len);
@@ -598,9 +616,16 @@ int kvm_riscv_vcpu_mmio_store(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		 * Bit[0] == 0 implies trapped instruction value is
 		 * zero or special value.
 		 */
-		insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc,
-						  &utrap);
+		if (vcpu->arch.is_svm) {
+			// this is a hack because we have difficulties setting up htinst
+			// from the M-mode. A workaround is to store htinst in t6.
+			insn = ct->t6;
+		} else {
+			insn = kvm_riscv_vcpu_unpriv_read(vcpu, true, ct->sepc, &utrap);
+		}
+		
 		if (utrap.scause) {
+			// printk(KERN_INFO "KVM store: redirect trap\n");
 			/* Redirect trap if we failed to read instruction */
 			utrap.sepc = ct->sepc;
 			kvm_riscv_vcpu_trap_redirect(vcpu, &utrap);
@@ -609,6 +634,10 @@ int kvm_riscv_vcpu_mmio_store(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		insn_len = INSN_LEN(insn);
 	}
 
+	// if (fault_addr < 0x10000000 || fault_addr > 0x10000003) {
+		// printk(KERN_INFO "KVM store: htinst=%lx, insn=%lx fault_addr=%lx\n", htinst, insn, fault_addr);
+	// }
+
 	data = GET_RS2(insn, &vcpu->arch.guest_context);
 	data8 = data16 = data32 = data64 = data;
 
@@ -623,18 +652,18 @@ int kvm_riscv_vcpu_mmio_store(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	} else if ((insn & INSN_MASK_SH) == INSN_MATCH_SH) {
 		len = 2;
 #ifdef CONFIG_64BIT
-	} else if ((insn & INSN_MASK_C_SD) == INSN_MATCH_C_SD) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_SD) == INSN_MATCH_C_SD) {
 		len = 8;
 		data64 = GET_RS2S(insn, &vcpu->arch.guest_context);
-	} else if ((insn & INSN_MASK_C_SDSP) == INSN_MATCH_C_SDSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_SDSP) == INSN_MATCH_C_SDSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 8;
 		data64 = GET_RS2C(insn, &vcpu->arch.guest_context);
 #endif
-	} else if ((insn & INSN_MASK_C_SW) == INSN_MATCH_C_SW) {
+	} else if (((insn & ~0b1) & INSN_MASK_C_SW) == INSN_MATCH_C_SW) {
 		len = 4;
 		data32 = GET_RS2S(insn, &vcpu->arch.guest_context);
-	} else if ((insn & INSN_MASK_C_SWSP) == INSN_MATCH_C_SWSP &&
+	} else if (((insn & ~0b1) & INSN_MASK_C_SWSP) == INSN_MATCH_C_SWSP &&
 		   ((insn >> SH_RD) & 0x1f)) {
 		len = 4;
 		data32 = GET_RS2C(insn, &vcpu->arch.guest_context);
@@ -644,7 +673,7 @@ int kvm_riscv_vcpu_mmio_store(struct kvm_vcpu *vcpu, struct kvm_run *run,
 
 	/* Fault address should be aligned to length of MMIO */
 	if (fault_addr & (len - 1))
-		return -EIO;
+		return -EIO;	
 
 	/* Save instruction decode info */
 	vcpu->arch.mmio_decode.insn = insn;
@@ -708,14 +737,16 @@ int kvm_riscv_vcpu_mmio_return(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	ulong insn;
 	int len, shift;
 
-	if (vcpu->arch.mmio_decode.return_handled)
+
+	if (vcpu->arch.mmio_decode.return_handled) {
 		return 0;
+	}
 
 	vcpu->arch.mmio_decode.return_handled = 1;
 	insn = vcpu->arch.mmio_decode.insn;
 
 	if (run->mmio.is_write)
-		goto done;
+		goto done;	
 
 	len = vcpu->arch.mmio_decode.len;
 	shift = vcpu->arch.mmio_decode.shift;
@@ -723,21 +754,25 @@ int kvm_riscv_vcpu_mmio_return(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	switch (len) {
 	case 1:
 		data8 = *((u8 *)run->mmio.data);
+		// printk(KERN_INFO "KVM MMIO return: %lx to REG_OFFSET(insn, SH_RD)=%d\n", data8, REG_OFFSET(insn, SH_RD));
 		SET_RD(insn, &vcpu->arch.guest_context,
 			(ulong)data8 << shift >> shift);
 		break;
 	case 2:
 		data16 = *((u16 *)run->mmio.data);
+		// printk(KERN_INFO "KVM MMIO return: %lx to REG_OFFSET(insn, SH_RD)=%d\n", data16, REG_OFFSET(insn, SH_RD));
 		SET_RD(insn, &vcpu->arch.guest_context,
 			(ulong)data16 << shift >> shift);
 		break;
 	case 4:
 		data32 = *((u32 *)run->mmio.data);
+		// printk(KERN_INFO "KVM MMIO return: %lx to REG_OFFSET(insn, SH_RD)=%d\n", data32, REG_OFFSET(insn, SH_RD));
 		SET_RD(insn, &vcpu->arch.guest_context,
 			(ulong)data32 << shift >> shift);
 		break;
 	case 8:
 		data64 = *((u64 *)run->mmio.data);
+		// printk(KERN_INFO "KVM MMIO return: %lx to REG_OFFSET(insn, SH_RD)=%d\n", data64, REG_OFFSET(insn, SH_RD));
 		SET_RD(insn, &vcpu->arch.guest_context,
 			(ulong)data64 << shift >> shift);
 		break;
diff --git a/arch/riscv/kvm/vcpu_sbi.c b/arch/riscv/kvm/vcpu_sbi.c
index 15fde15f9fb8..a17c23dd93bd 100644
--- a/arch/riscv/kvm/vcpu_sbi.c
+++ b/arch/riscv/kvm/vcpu_sbi.c
@@ -41,6 +41,7 @@ static const struct kvm_vcpu_sbi_extension *sbi_ext[] = {
 	&vcpu_sbi_ext_pmu,
 	&vcpu_sbi_ext_experimental,
 	&vcpu_sbi_ext_vendor,
+	&vcpu_sbi_ext_ace,
 };
 
 void kvm_riscv_vcpu_sbi_forward(struct kvm_vcpu *vcpu, struct kvm_run *run)
@@ -69,6 +70,12 @@ void kvm_riscv_vcpu_sbi_system_reset(struct kvm_vcpu *vcpu,
 	unsigned long i;
 	struct kvm_vcpu *tmp;
 
+	if (vcpu->arch.is_svm) {
+		sbi_ecall(0x510000, 3001, vcpu->arch.svm_id, 0, 0, 0, 0, 0);
+		vcpu->arch.is_svm = 0;
+		printk(KERN_INFO "ACE KVM: SVM terminated\n");
+	}
+
 	kvm_for_each_vcpu(i, tmp, vcpu->kvm)
 		tmp->arch.power_off = true;
 	kvm_make_all_cpus_request(vcpu->kvm, KVM_REQ_SLEEP);
@@ -127,6 +134,9 @@ int kvm_riscv_vcpu_sbi_ecall(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	bool ext_is_v01 = false;
 
 	sbi_ext = kvm_vcpu_sbi_find_ext(cp->a7);
+
+	// kvm_err("kvm_riscv_vcpu_sbi_ecall: :%ld %ld\n", sbi_ext, cp->a7);
+
 	if (sbi_ext && sbi_ext->handler) {
 #ifdef CONFIG_RISCV_SBI_V01
 		if (cp->a7 >= SBI_EXT_0_1_SET_TIMER &&
@@ -134,7 +144,7 @@ int kvm_riscv_vcpu_sbi_ecall(struct kvm_vcpu *vcpu, struct kvm_run *run)
 			ext_is_v01 = true;
 #endif
 		ret = sbi_ext->handler(vcpu, run, &sbi_ret);
-	} else {
+	} else {		
 		/* Return error for unsupported SBI calls */
 		cp->a0 = SBI_ERR_NOT_SUPPORTED;
 		goto ecall_done;
diff --git a/arch/riscv/kvm/vcpu_sbi_ace.c b/arch/riscv/kvm/vcpu_sbi_ace.c
new file mode 100644
index 000000000000..223ad1997d71
--- /dev/null
+++ b/arch/riscv/kvm/vcpu_sbi_ace.c
@@ -0,0 +1,140 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2021 IBM.
+ *
+ * Authors:
+ *     Wojciech Ozga <woz@zurich.ibm.com>
+ */
+
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <linux/kvm_host.h>
+#include <asm/sbi.h>
+#include <asm/kvm_vcpu_sbi.h>
+#include <asm/kvm_host.h>
+#include <asm/io.h>
+
+const int SECURITY_MONITOR_EXTID = 0x510000;
+const int SECURITY_MONITOR_PAGE_IN_FID = 2003;
+
+const int SBI_EXT_ACE_LOAD_ALL_PAGES = 0;
+const int SBI_EXT_ACE_REGISTER_SVM = 1;
+const int SBI_EXT_ACE_PAGE_IN = 2;
+
+phys_addr_t test_phys_addr = 0;
+
+static int kvm_sbi_ace_load_all_pages(struct kvm_vcpu *vcpu)
+{
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	struct kvm_memory_slot *memslot;
+	unsigned long hva, fault_addr;
+	bool writable;
+	gfn_t gfn;
+	int ret;
+	long page;
+
+	printk(KERN_INFO "ACE KVM: loading all pages\n");
+	long memory_start = 0x80000000;
+	long memory_size = 128*1024*1024; // TODO: get the real value
+	long number_of_pages = memory_size / 4096;
+	for (page=0; page<number_of_pages; page++) {
+		fault_addr = memory_start + page * 4096;
+		gfn = fault_addr >> PAGE_SHIFT;
+		memslot = gfn_to_memslot(vcpu->kvm, gfn);
+		hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+		if (memslot != NULL && !kvm_is_error_hva(hva)) {
+			kvm_riscv_gstage_map(vcpu, memslot, fault_addr, hva, true);
+		}
+	}
+
+	return 0;
+}
+
+static int kvm_sbi_ace_register_svm(struct kvm_vcpu *vcpu)
+{
+	struct kvm *kvm = vcpu->kvm;
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	unsigned long svm_id = cp->a0;
+	unsigned long vcpu_id = cp->a1;
+
+	vcpu->arch.is_svm = 1;
+	vcpu->arch.svm_id = svm_id;
+	vcpu->arch.vcpu_id = vcpu_id;
+
+	printk(KERN_INFO "ACE KVM: registered SVM[id=%ld]\n", svm_id);
+
+	return 0;
+}
+
+static int kvm_sbi_ace_page_in(struct kvm_vcpu *vcpu, struct kvm_vcpu_sbi_return *retdata)
+{
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	unsigned long svm_id = vcpu->arch.svm_id;
+	unsigned long hart_id = vcpu->arch.vcpu_id;
+	unsigned long virt_addr = cp->a0;	
+	unsigned long is_error = 0;
+	int page_size = 4096;
+	struct sbiret result;
+	bool writable;
+
+	gpa_t gpa = virt_addr;
+	gfn_t gfn = gpa >> PAGE_SHIFT;
+
+	struct kvm_memory_slot *memslot = gfn_to_memslot(vcpu->kvm, gfn);
+	phys_addr_t hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+	kvm_riscv_gstage_map(vcpu, memslot, gpa, hva, true);
+	unsigned long hfn = gfn_to_pfn_prot(vcpu->kvm, gfn, true, NULL);
+	phys_addr_t hpa = hfn << PAGE_SHIFT;
+
+	struct kvm_cpu_context *reset_cntx = &vcpu->arch.guest_reset_context;
+	// reset_cntx = &target_vcpu->arch.;
+	// reset_cntx->sepc = cp->a1;	
+
+	retdata->out_val = hpa;
+
+	// result = sbi_ecall(SECURITY_MONITOR_EXTID, SECURITY_MONITOR_PAGE_IN_FID, cookie, is_error, svm_id, hart_id, hpa, 0);
+	// if (result.error > 0) {
+	// 	printk(KERN_INFO "KVM: Security Monitor returned error from PAGE_IN sm-call.\n");
+	// }
+
+	return 0;
+}
+
+static int kvm_sbi_ext_ace_handler(struct kvm_vcpu *vcpu, struct kvm_run *run,
+				   struct kvm_vcpu_sbi_return *retdata)
+{
+	int ret = 0;
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	struct kvm *kvm = vcpu->kvm;
+	unsigned long funcid = cp->a6;
+
+	switch (funcid) {
+	case SBI_EXT_ACE_LOAD_ALL_PAGES:
+		mutex_lock(&kvm->lock);
+		ret = kvm_sbi_ace_load_all_pages(vcpu);
+		mutex_unlock(&kvm->lock);
+		break;
+	case SBI_EXT_ACE_REGISTER_SVM:
+		mutex_lock(&kvm->lock);
+		ret = kvm_sbi_ace_register_svm(vcpu);
+		mutex_unlock(&kvm->lock);
+		break;
+	case SBI_EXT_ACE_PAGE_IN:
+		mutex_lock(&kvm->lock);
+		ret = kvm_sbi_ace_page_in(vcpu, retdata);
+		mutex_unlock(&kvm->lock);
+		break;
+	default:
+		ret = SBI_ERR_NOT_SUPPORTED;
+	}
+
+	retdata->err_val = ret;
+
+	return 0;
+}
+
+const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_ace = {
+	.extid_start = SBI_EXT_ACE,
+	.extid_end = SBI_EXT_ACE,
+	.handler = kvm_sbi_ext_ace_handler,
+};
diff --git a/arch/riscv/kvm/vcpu_switch.S b/arch/riscv/kvm/vcpu_switch.S
index d74df8eb4d71..5e25d05ef969 100644
--- a/arch/riscv/kvm/vcpu_switch.S
+++ b/arch/riscv/kvm/vcpu_switch.S
@@ -210,6 +210,216 @@ __kvm_switch_return:
 	ret
 ENDPROC(__kvm_riscv_switch_to)
 
+# ACE START, 
+#	a0 - address of the vcpu->arch
+#	a1 - SM-call function ID
+#   a2 - first argument of the SM-call
+ENTRY(__kvm_riscv_ace_switch_to)
+	/* Save Host GPRs (except A0 and T0-T6) */
+	REG_S	ra, (KVM_ARCH_HOST_RA)(a0)
+	REG_S	sp, (KVM_ARCH_HOST_SP)(a0)
+	REG_S	gp, (KVM_ARCH_HOST_GP)(a0)
+	REG_S	tp, (KVM_ARCH_HOST_TP)(a0)
+	REG_S	s0, (KVM_ARCH_HOST_S0)(a0)
+	REG_S	s1, (KVM_ARCH_HOST_S1)(a0)
+	REG_S	a1, (KVM_ARCH_HOST_A1)(a0)
+	REG_S	a2, (KVM_ARCH_HOST_A2)(a0)
+	REG_S	a3, (KVM_ARCH_HOST_A3)(a0)
+	REG_S	a4, (KVM_ARCH_HOST_A4)(a0)
+	REG_S	a5, (KVM_ARCH_HOST_A5)(a0)
+	REG_S	a6, (KVM_ARCH_HOST_A6)(a0)
+	REG_S	a7, (KVM_ARCH_HOST_A7)(a0)
+	REG_S	s2, (KVM_ARCH_HOST_S2)(a0)
+	REG_S	s3, (KVM_ARCH_HOST_S3)(a0)
+	REG_S	s4, (KVM_ARCH_HOST_S4)(a0)
+	REG_S	s5, (KVM_ARCH_HOST_S5)(a0)
+	REG_S	s6, (KVM_ARCH_HOST_S6)(a0)
+	REG_S	s7, (KVM_ARCH_HOST_S7)(a0)
+	REG_S	s8, (KVM_ARCH_HOST_S8)(a0)
+	REG_S	s9, (KVM_ARCH_HOST_S9)(a0)
+	REG_S	s10, (KVM_ARCH_HOST_S10)(a0)
+	REG_S	s11, (KVM_ARCH_HOST_S11)(a0)
+
+	/* Load Guest CSR values */
+	REG_L	t0, (KVM_ARCH_GUEST_SSTATUS)(a0)
+	REG_L	t1, (KVM_ARCH_GUEST_HSTATUS)(a0)
+	REG_L	t2, (KVM_ARCH_GUEST_SCOUNTEREN)(a0)
+	la	t4, __kvm_ace_switch_return
+	REG_L	t5, (KVM_ARCH_GUEST_SEPC)(a0)
+
+	/* Save Host and Restore Guest SSTATUS */
+	csrrw	t0, CSR_SSTATUS, t0
+
+	/* Save Host and Restore Guest HSTATUS */
+	csrrw	t1, CSR_HSTATUS, t1
+
+	/* Save Host and Restore Guest SCOUNTEREN */
+	csrrw	t2, CSR_SCOUNTEREN, t2
+
+	/* Save Host STVEC and change it to return path */
+	csrrw	t4, CSR_STVEC, t4
+
+	/* Save Host SSCRATCH and change it to struct kvm_vcpu_arch pointer */
+	csrrw	t3, CSR_SSCRATCH, a0
+
+	/* Restore Guest SEPC */
+	csrw	CSR_SEPC, t5
+
+	/* Store Host CSR values */
+	REG_S	t0, (KVM_ARCH_HOST_SSTATUS)(a0)
+	REG_S	t1, (KVM_ARCH_HOST_HSTATUS)(a0)
+	REG_S	t2, (KVM_ARCH_HOST_SCOUNTEREN)(a0)
+	REG_S	t3, (KVM_ARCH_HOST_SSCRATCH)(a0)
+	REG_S	t4, (KVM_ARCH_HOST_STVEC)(a0)
+
+	# invoke security monitor resume sm-call
+	li		a7, 0x510000 # ACE_EXT_ID that identifies SM-call
+	add		a6, a1, 0	 # function ID
+	add		t0, a2, 0	 # first argument of the SM-call
+	add		t1, a3, 0	 # 2nd argument of the SM-call
+	add		t2, a4, 0	 # 3rd argument of the SM-call
+	add		t3, a5, 0	 # 4th argument of the SM-call	
+
+	/* Restore Guest GPRs (except A0) */
+	REG_L	ra, (KVM_ARCH_GUEST_RA)(a0)
+	REG_L	sp, (KVM_ARCH_GUEST_SP)(a0)
+	REG_L	gp, (KVM_ARCH_GUEST_GP)(a0)
+	REG_L	tp, (KVM_ARCH_GUEST_TP)(a0)
+	# our convention: we use t0-t5 as arguments to ACE
+	# because a0-a5 is used by KVM for hcalls, mmio etc	
+	# REG_L	t0, (KVM_ARCH_GUEST_T0)(a0)
+	# REG_L	t1, (KVM_ARCH_GUEST_T1)(a0)
+	# REG_L	t2, (KVM_ARCH_GUEST_T2)(a0)
+	REG_L	s0, (KVM_ARCH_GUEST_S0)(a0)
+	REG_L	s1, (KVM_ARCH_GUEST_S1)(a0)
+	REG_L	a1, (KVM_ARCH_GUEST_A1)(a0)
+	REG_L	a2, (KVM_ARCH_GUEST_A2)(a0)
+	REG_L	a3, (KVM_ARCH_GUEST_A3)(a0)
+	REG_L	a4, (KVM_ARCH_GUEST_A4)(a0)
+	REG_L	a5, (KVM_ARCH_GUEST_A5)(a0)
+	# REG_L	a6, (KVM_ARCH_GUEST_A6)(a0)
+	# REG_L	a7, (KVM_ARCH_GUEST_A7)(a0)
+	REG_L	s2, (KVM_ARCH_GUEST_S2)(a0)
+	REG_L	s3, (KVM_ARCH_GUEST_S3)(a0)
+	REG_L	s4, (KVM_ARCH_GUEST_S4)(a0)
+	REG_L	s5, (KVM_ARCH_GUEST_S5)(a0)
+	REG_L	s6, (KVM_ARCH_GUEST_S6)(a0)
+	REG_L	s7, (KVM_ARCH_GUEST_S7)(a0)
+	REG_L	s8, (KVM_ARCH_GUEST_S8)(a0)
+	REG_L	s9, (KVM_ARCH_GUEST_S9)(a0)
+	REG_L	s10, (KVM_ARCH_GUEST_S10)(a0)
+	REG_L	s11, (KVM_ARCH_GUEST_S11)(a0)
+	# REG_L	t3, (KVM_ARCH_GUEST_T3)(a0)
+	# REG_L	t4, (KVM_ARCH_GUEST_T4)(a0)
+	# REG_L	t5, (KVM_ARCH_GUEST_T5)(a0)
+	# REG_L	t6, (KVM_ARCH_GUEST_T6)(a0)
+
+	REG_L	a0, (KVM_ARCH_GUEST_A0)(a0)
+
+	/* Resume Guest */
+	ecall
+
+	/* Back to Host */
+	.align 4
+__kvm_ace_switch_return:
+	/* Swap Guest A0 with SSCRATCH */
+	csrrw	a0, CSR_SSCRATCH, a0
+
+	/* Save Guest GPRs (except A0) */
+	REG_S	ra, (KVM_ARCH_GUEST_RA)(a0)
+	REG_S	sp, (KVM_ARCH_GUEST_SP)(a0)
+	REG_S	gp, (KVM_ARCH_GUEST_GP)(a0)
+	REG_S	tp, (KVM_ARCH_GUEST_TP)(a0)
+	REG_S	t0, (KVM_ARCH_GUEST_T0)(a0)
+	REG_S	t1, (KVM_ARCH_GUEST_T1)(a0)
+	REG_S	t2, (KVM_ARCH_GUEST_T2)(a0)
+	REG_S	s0, (KVM_ARCH_GUEST_S0)(a0)
+	REG_S	s1, (KVM_ARCH_GUEST_S1)(a0)
+	REG_S	a1, (KVM_ARCH_GUEST_A1)(a0)
+	REG_S	a2, (KVM_ARCH_GUEST_A2)(a0)
+	REG_S	a3, (KVM_ARCH_GUEST_A3)(a0)
+	REG_S	a4, (KVM_ARCH_GUEST_A4)(a0)
+	REG_S	a5, (KVM_ARCH_GUEST_A5)(a0)
+	REG_S	a6, (KVM_ARCH_GUEST_A6)(a0)
+	REG_S	a7, (KVM_ARCH_GUEST_A7)(a0)
+	REG_S	s2, (KVM_ARCH_GUEST_S2)(a0)
+	REG_S	s3, (KVM_ARCH_GUEST_S3)(a0)
+	REG_S	s4, (KVM_ARCH_GUEST_S4)(a0)
+	REG_S	s5, (KVM_ARCH_GUEST_S5)(a0)
+	REG_S	s6, (KVM_ARCH_GUEST_S6)(a0)
+	REG_S	s7, (KVM_ARCH_GUEST_S7)(a0)
+	REG_S	s8, (KVM_ARCH_GUEST_S8)(a0)
+	REG_S	s9, (KVM_ARCH_GUEST_S9)(a0)
+	REG_S	s10, (KVM_ARCH_GUEST_S10)(a0)
+	REG_S	s11, (KVM_ARCH_GUEST_S11)(a0)
+	REG_S	t3, (KVM_ARCH_GUEST_T3)(a0)
+	REG_S	t4, (KVM_ARCH_GUEST_T4)(a0)
+	REG_S	t5, (KVM_ARCH_GUEST_T5)(a0)
+	REG_S	t6, (KVM_ARCH_GUEST_T6)(a0)
+
+	/* Load Host CSR values */
+	REG_L	t1, (KVM_ARCH_HOST_STVEC)(a0)
+	REG_L	t2, (KVM_ARCH_HOST_SSCRATCH)(a0)
+	REG_L	t3, (KVM_ARCH_HOST_SCOUNTEREN)(a0)
+	REG_L	t4, (KVM_ARCH_HOST_HSTATUS)(a0)
+	REG_L	t5, (KVM_ARCH_HOST_SSTATUS)(a0)
+
+	/* Save Guest SEPC */
+	csrr	t0, CSR_SEPC
+
+	/* Save Guest A0 and Restore Host SSCRATCH */
+	csrrw	t2, CSR_SSCRATCH, t2
+
+	/* Restore Host STVEC */
+	csrw	CSR_STVEC, t1
+
+	/* Save Guest and Restore Host SCOUNTEREN */
+	csrrw	t3, CSR_SCOUNTEREN, t3
+
+	/* Save Guest and Restore Host HSTATUS */
+	csrrw	t4, CSR_HSTATUS, t4
+
+	/* Save Guest and Restore Host SSTATUS */
+	csrrw	t5, CSR_SSTATUS, t5
+
+	/* Store Guest CSR values */
+	REG_S	t0, (KVM_ARCH_GUEST_SEPC)(a0)
+	REG_S	t2, (KVM_ARCH_GUEST_A0)(a0)
+	REG_S	t3, (KVM_ARCH_GUEST_SCOUNTEREN)(a0)
+	REG_S	t4, (KVM_ARCH_GUEST_HSTATUS)(a0)
+	REG_S	t5, (KVM_ARCH_GUEST_SSTATUS)(a0)
+
+	/* Restore Host GPRs (except A0 and T0-T6) */
+	REG_L	ra, (KVM_ARCH_HOST_RA)(a0)
+	REG_L	sp, (KVM_ARCH_HOST_SP)(a0)
+	REG_L	gp, (KVM_ARCH_HOST_GP)(a0)
+	REG_L	tp, (KVM_ARCH_HOST_TP)(a0)
+	REG_L	s0, (KVM_ARCH_HOST_S0)(a0)
+	REG_L	s1, (KVM_ARCH_HOST_S1)(a0)
+	REG_L	a1, (KVM_ARCH_HOST_A1)(a0)
+	REG_L	a2, (KVM_ARCH_HOST_A2)(a0)
+	REG_L	a3, (KVM_ARCH_HOST_A3)(a0)
+	REG_L	a4, (KVM_ARCH_HOST_A4)(a0)
+	REG_L	a5, (KVM_ARCH_HOST_A5)(a0)
+	REG_L	a6, (KVM_ARCH_HOST_A6)(a0)
+	REG_L	a7, (KVM_ARCH_HOST_A7)(a0)
+	REG_L	s2, (KVM_ARCH_HOST_S2)(a0)
+	REG_L	s3, (KVM_ARCH_HOST_S3)(a0)
+	REG_L	s4, (KVM_ARCH_HOST_S4)(a0)
+	REG_L	s5, (KVM_ARCH_HOST_S5)(a0)
+	REG_L	s6, (KVM_ARCH_HOST_S6)(a0)
+	REG_L	s7, (KVM_ARCH_HOST_S7)(a0)
+	REG_L	s8, (KVM_ARCH_HOST_S8)(a0)
+	REG_L	s9, (KVM_ARCH_HOST_S9)(a0)
+	REG_L	s10, (KVM_ARCH_HOST_S10)(a0)
+	REG_L	s11, (KVM_ARCH_HOST_S11)(a0)
+
+	/* Return to C code */
+	ret
+ENDPROC(__kvm_riscv_ace_switch_to)
+
+# ACE END
+
 ENTRY(__kvm_riscv_unpriv_trap)
 	/*
 	 * We assume that faulting unpriv load/store instruction is
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index d255964ec331..211c795b2c03 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -4095,7 +4095,7 @@ static long kvm_vcpu_ioctl(struct file *filp,
 			if (oldpid)
 				synchronize_rcu();
 			put_pid(oldpid);
-		}
+		}		
 		r = kvm_arch_vcpu_ioctl_run(vcpu);
 		trace_kvm_userspace_exit(vcpu->run->exit_reason, r);
 		break;
